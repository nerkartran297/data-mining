import pandas as pd
import numpy as np
from collections import Counter
import math
from itertools import combinations
import streamlit as st 

# Th∆∞ vi·ªán cho c√°c thu·∫≠t to√°n c·ª• th·ªÉ
from mlxtend.frequent_patterns import apriori, association_rules
from sklearn.naive_bayes import MultinomialNB, GaussianNB
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.cluster import KMeans
from minisom import MiniSom
import graphviz 

# ==============================================================================
# PH·∫¶N 1: LOGIC T·ª™ SCRIPT APRIORI
# ==============================================================================
def apriori_original_process_data(df_input):
    """H√†m process_data g·ªëc t·ª´ script Apriori c·ªßa b·∫°n (ch·ªâ ch·ªçn c·ªôt s·ªë)."""
    # st.warning("ƒêang s·ª≠ d·ª•ng `process_data` g·ªëc cho Apriori: ch·ªâ ch·ªçn c√°c c·ªôt s·ªë...") # Th√¥ng b√°o n√†y n√™n ·ªü app.py
    return df_input.select_dtypes(include=['number'])

def apriori_general_transactional_conversion(df_input):
    """Chuy·ªÉn ƒë·ªïi DataFrame chung th√†nh d·∫°ng boolean cho Apriori."""
    # st.info("ƒêang chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu sang d·∫°ng boolean cho Apriori...") # Th√¥ng b√°o n√†y n√™n ·ªü app.py
    return df_input.applymap(lambda x: True if pd.notna(x) and x != 0 and x != '' and x is not False else False)

def apriori_one_hot_encode_data(df_input, selected_columns):
    """H√†m ti·ªÅn x·ª≠ l√Ω b·∫±ng one-hot encoding cho Apriori."""
    if not selected_columns:
        raise ValueError("Ph·∫£i ch·ªçn √≠t nh·∫•t m·ªôt c·ªôt ƒë·ªÉ one-hot encode.")
    
    # N·∫øu c√≥ ƒë√∫ng 2 c·ªôt, s·ª≠ d·ª•ng crosstab (transaction format)
    if len(selected_columns) == 2:
        invoice_col, item_col = selected_columns
        df_encoded = pd.crosstab(df_input[invoice_col], df_input[item_col]).T.astype(bool)
        return df_encoded
    
    # N·∫øu c√≥ nhi·ªÅu c·ªôt, s·ª≠ d·ª•ng one-hot encoding th√¥ng th∆∞·ªùng
    df_encoded = pd.get_dummies(df_input[selected_columns], prefix_sep='_').astype(bool)
    return df_encoded

def apriori_get_maximal_frequent_itemsets(frequent_itemsets_df):
    maximal_itemsets_list = []
    if frequent_itemsets_df.empty:
        return pd.DataFrame(maximal_itemsets_list, columns=frequent_itemsets_df.columns if not frequent_itemsets_df.empty else ['support', 'itemsets'])

    for i, row in frequent_itemsets_df.iterrows():
        current_itemset = row['itemsets']
        is_maximal = True
        for j, other_row in frequent_itemsets_df.iterrows():
            if i == j:
                continue
            other_itemset = other_row['itemsets']
            if current_itemset.issubset(other_itemset) and current_itemset != other_itemset:
                is_maximal = False
                break
        if is_maximal:
            maximal_itemsets_list.append(row)
    return pd.DataFrame(maximal_itemsets_list) if maximal_itemsets_list else pd.DataFrame(columns=frequent_itemsets_df.columns)


def run_apriori_calculations(df_processed, min_sup, min_conf):
    """Ch·∫°y thu·∫≠t to√°n Apriori v√† tr·∫£ v·ªÅ k·∫øt qu·∫£."""
    frequent_itemsets_df = pd.DataFrame()
    rules_df = pd.DataFrame()
    maximal_frequent_itemsets_df = pd.DataFrame()

    if df_processed.empty or not any(df_processed.any()):
        raise ValueError("D·ªØ li·ªáu x·ª≠ l√Ω cho Apriori r·ªóng ho·∫∑c kh√¥ng ch·ª©a gi√° tr·ªã True n√†o.")

    frequent_itemsets_df = apriori(df_processed, min_support=min_sup, use_colnames=True)

    if frequent_itemsets_df.empty:
        return frequent_itemsets_df, rules_df, maximal_frequent_itemsets_df 

    maximal_frequent_itemsets_df = apriori_get_maximal_frequent_itemsets(frequent_itemsets_df.copy())
    
    # Sinh lu·∫≠t t·ª´ t·∫•t c·∫£ t·∫≠p ph·ªï bi·∫øn
    if not frequent_itemsets_df.empty:
        rules_df = association_rules(frequent_itemsets_df, metric="confidence", min_threshold=min_conf)
    
    return frequent_itemsets_df, rules_df, maximal_frequent_itemsets_df

# ==============================================================================
# PH·∫¶N 2: LOGIC T·ª™ SCRIPT ROUGH SET
# ==============================================================================
def rs_lower_approximation(df, target_class_value, condition_attributes, decision_attribute_name):
    X = set(df[df[decision_attribute_name] == target_class_value].index)
    condition_attributes_list = list(condition_attributes)
    if not condition_attributes_list:
        return X if all(df[decision_attribute_name] == target_class_value) else set()

    partitions = df.groupby(condition_attributes_list).groups.values()
    lower_approx = set()
    for block_indices in partitions:
        block_set = set(block_indices)
        if block_set.issubset(X):
            lower_approx.update(block_set)
    return lower_approx

def rs_upper_approximation(df, target_class_value, condition_attributes, decision_attribute_name):
    X = set(df[df[decision_attribute_name] == target_class_value].index)
    condition_attributes_list = list(condition_attributes)
    if not condition_attributes_list:
        return set(df.index) if X else set()

    partitions = df.groupby(condition_attributes_list).groups.values()
    upper_approx = set()
    for block_indices in partitions:
        block_set = set(block_indices)
        if block_set & X:
            upper_approx.update(block_set)
    return upper_approx

def rs_accuracy(df, target_class_value, condition_attributes, decision_attribute_name):
    lower = rs_lower_approximation(df, target_class_value, condition_attributes, decision_attribute_name)
    upper = rs_upper_approximation(df, target_class_value, condition_attributes, decision_attribute_name)
    return len(lower) / len(upper) if len(upper) > 0 else 0

def rs_dependency(df, condition_attributes, decision_attribute_name):
    condition_attributes_list = list(condition_attributes)
    if not condition_attributes_list:
        return 0
        
    partitions = df.groupby(condition_attributes_list).groups.values()
    total_len = len(df)
    if total_len == 0: return 0
    
    dependency_sum = sum(len(list(block_indices)) for block_indices in partitions if len(df.loc[list(block_indices), decision_attribute_name].unique()) == 1)
    return dependency_sum / total_len

def rs_find_all_reducts(df, all_possible_condition_attributes, decision_attribute_name):
    all_possible_condition_attributes = list(all_possible_condition_attributes)
    all_reducts = []
    full_dependency = rs_dependency(df, all_possible_condition_attributes, decision_attribute_name)

    for r_len in range(1, len(all_possible_condition_attributes) + 1):
        for subset_tuple in combinations(all_possible_condition_attributes, r_len):
            subset_list = list(subset_tuple)
            if rs_dependency(df, subset_list, decision_attribute_name) == full_dependency:
                is_minimal = True
                for existing_reduct in all_reducts:
                    if set(existing_reduct).issubset(set(subset_list)) and len(existing_reduct) < len(subset_list):
                        is_minimal = False
                        break
                if is_minimal:
                    all_reducts = [red for red in all_reducts if not (set(subset_list).issubset(set(red)) and len(subset_list) < len(red))]
                    if subset_list not in all_reducts:
                         all_reducts.append(subset_list)
    return all_reducts if all_reducts else []

# ==============================================================================
# PH·∫¶N 3: LOGIC T·ª™ SCRIPT C√ÇY QUY·∫æT ƒê·ªäNH (ID3)
# ==============================================================================
def dt_entropy(column_values):
    counts = Counter(column_values)
    total_count = len(column_values)
    if total_count == 0: return 0
    return -sum((count / total_count) * math.log2(count / total_count) for count in counts.values() if count > 0)

def dt_gini_impurity_for_attribute_split(df_subset, target_attribute_name):
    total_count = len(df_subset)
    if total_count == 0: return 0
    impurity = 1.0
    for class_val in df_subset[target_attribute_name].unique():
        p_i = len(df_subset[df_subset[target_attribute_name] == class_val]) / total_count
        impurity -= p_i ** 2
    return impurity

def dt_info_gain(df, split_attribute_name, target_attribute_name):
    total_entropy_val = dt_entropy(df[target_attribute_name])
    weighted_entropy_after_split = 0
    total_len = len(df)
    if total_len == 0: return 0
    for value in df[split_attribute_name].unique():
        subset = df[df[split_attribute_name] == value]
        subset_len = len(subset)
        if subset_len > 0:
            weighted_entropy_after_split += (subset_len / total_len) * dt_entropy(subset[target_attribute_name])
    return total_entropy_val - weighted_entropy_after_split

def dt_gini_gain(df, split_attribute_name, target_attribute_name):
    parent_gini = dt_gini_impurity_for_attribute_split(df, target_attribute_name)
    weighted_child_gini = 0
    total_len = len(df)
    if total_len == 0: return 0
    for value in df[split_attribute_name].unique():
        subset = df[df[split_attribute_name] == value]
        subset_len = len(subset)
        if subset_len > 0:
            weighted_child_gini += (subset_len / total_len) * dt_gini_impurity_for_attribute_split(subset, target_attribute_name)
    return parent_gini - weighted_child_gini

class DecisionTreeNode:
    def __init__(self, attribute=None, results=None, branches=None, attribute_value_from_parent=None):
        self.attribute = attribute
        self.results = results
        self.branches = branches if branches else {}
        self.attribute_value_from_parent = attribute_value_from_parent

def dt_build_tree(df, current_attributes, target_attribute_name, method='Gain'):
    if len(df[target_attribute_name].unique()) == 1:
        return DecisionTreeNode(results=df[target_attribute_name].iloc[0])
    if not current_attributes or df.empty:
        if df.empty:
            return DecisionTreeNode(results="Kh√¥ng x√°c ƒë·ªãnh (nh√°nh r·ªóng)") 
        mode_values = df[target_attribute_name].mode()
        majority_class = mode_values.iloc[0] if not mode_values.empty else df[target_attribute_name].iloc[0]
        return DecisionTreeNode(results=majority_class)

    best_attribute_to_split = None
    best_score = -1
    for attr in current_attributes:
        if method == 'Gain':
            score = dt_info_gain(df, attr, target_attribute_name)
        else:
            score = dt_gini_gain(df, attr, target_attribute_name)
        if score > best_score:
            best_score = score
            best_attribute_to_split = attr
    
    if best_attribute_to_split is None or best_score <= 0:
        mode_values = df[target_attribute_name].mode()
        majority_class = mode_values.iloc[0] if not mode_values.empty else df[target_attribute_name].iloc[0]
        return DecisionTreeNode(results=majority_class)

    tree_node = DecisionTreeNode(attribute=best_attribute_to_split)
    remaining_attributes_for_children = [attr for attr in current_attributes if attr != best_attribute_to_split]
    for value in df[best_attribute_to_split].dropna().unique():
        subset_df = df[df[best_attribute_to_split] == value].drop(columns=[best_attribute_to_split])
        subtree = dt_build_tree(subset_df, remaining_attributes_for_children, target_attribute_name, method)
        subtree.attribute_value_from_parent = value
        tree_node.branches[value] = subtree
    return tree_node

def dt_draw_tree_graphviz(node, target_attribute_name_for_display, dot=None, parent_id=None):
    def traverse(current_node, current_dot, current_parent_id=None, edge_label=""):
        node_id = str(id(current_node))
        if current_node.results is not None:
            # Leaf nodes (results) - green background with black text
            current_dot.node(node_id, f"{target_attribute_name_for_display} = {current_node.results}", 
                           shape="box", style="filled", color="lightgreen", fontcolor="black")
        else:
            # Internal nodes (attributes) - blue background with black text
            current_dot.node(node_id, str(current_node.attribute), 
                           shape="ellipse", style="filled", color="skyblue", fontcolor="black")
        if current_parent_id:
            # Edges with black text for labels
            current_dot.edge(current_parent_id, node_id, label=str(edge_label), fontcolor="black")
        for val, branch_node in current_node.branches.items():
            traverse(branch_node, current_dot, node_id, str(val))
        return current_dot

    if dot is None:
        dot = graphviz.Digraph(comment='Decision Tree')
        # Set graph background to white to ensure good contrast
        dot.attr(bgcolor='white')
        # Set default node attributes
        dot.attr('node', shape='ellipse', style='filled', color='skyblue', fontcolor='black')
        # Set default edge attributes  
        dot.attr('edge', color='black', fontcolor='black')
    return traverse(node, dot, parent_id)

def dt_extract_rules(node, target_attribute_name_for_display, current_rule_parts=None, rules_list=None, attribute_name_map=None):
    if current_rule_parts is None: current_rule_parts = []
    if rules_list is None: rules_list = []
    
    node_attribute_display = node.attribute
    if attribute_name_map and node.attribute in attribute_name_map:
        node_attribute_display = attribute_name_map[node.attribute]

    if node.results is not None:
        rule_str = "N·∫æU " + " V√Ä ".join(current_rule_parts) + f" TH√å {target_attribute_name_for_display} = {node.results}" if current_rule_parts else f"T·∫•t c·∫£ ƒë·ªÅu: {target_attribute_name_for_display} = {node.results}"
        rules_list.append(rule_str)
        return rules_list

    for value, subtree_node in node.branches.items():
        condition = f"{node_attribute_display} = '{value}'"
        dt_extract_rules(subtree_node, target_attribute_name_for_display, current_rule_parts + [condition], rules_list, attribute_name_map)
    return rules_list

# ==============================================================================
# PH·∫¶N 4: LOGIC T·ª™ SCRIPT NAIVE BAYES, K-MEANS, KOHONEN
# ==============================================================================
def general_preprocess_data_nbkm(df_input, target_column_name_for_nb=None, st_instance=None):

    df_processed = df_input.copy()
    
    if "Day" in df_processed.columns:
        df_processed.drop(columns=["Day"], inplace=True, errors='ignore')
    if "Tranh" in df_processed.columns and df_processed['Tranh'].nunique() == len(df_processed):
        if st_instance: st_instance.sidebar.warning("C·ªôt 'Tranh' ƒë∆∞·ª£c ph√°t hi·ªán v√† lo·∫°i b·ªè (coi nh∆∞ ID).")
        df_processed.drop(columns=["Tranh"], inplace=True, errors='ignore')
    
    df_processed.dropna(inplace=True)

    if df_processed.empty:
        
        return None, None, None, None


    X_temp = None
    y_processed_nb = None
    le_y_for_nb = None
    
    if target_column_name_for_nb and target_column_name_for_nb in df_processed.columns:
        X_temp = df_processed.drop(columns=[target_column_name_for_nb])
        y_series_nb = df_processed[target_column_name_for_nb]
        if y_series_nb.dtype == "object" or pd.api.types.is_categorical_dtype(y_series_nb):
            le_y_for_nb = LabelEncoder()
            y_processed_nb = le_y_for_nb.fit_transform(y_series_nb)
        else:
            y_processed_nb = y_series_nb.values 
    else:
        X_temp = df_processed.copy()

    feature_label_encoders = {}
    X_processed_features = X_temp.copy()

    for col in X_temp.columns:
        if X_temp[col].dtype == "object" or pd.api.types.is_categorical_dtype(X_temp[col]):
            try:
                X_processed_features[col] = pd.to_numeric(X_temp[col])
                if st_instance: st_instance.info(f"C·ªôt '{col}' (object/category) ƒë√£ ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi th√†nh s·ªë.")
            except ValueError:
                le = LabelEncoder()
                X_processed_features[col] = le.fit_transform(X_temp[col])
                feature_label_encoders[col] = le
                if st_instance: st_instance.info(f"C·ªôt '{col}' (object/category) ƒë√£ ƒë∆∞·ª£c m√£ h√≥a b·∫±ng LabelEncoder.")
        elif not pd.api.types.is_numeric_dtype(X_temp[col]):
            try:
                X_processed_features[col] = X_temp[col].astype(float)
            except ValueError:
                 if st_instance: st_instance.error(f"C·ªôt '{col}' c√≥ ki·ªÉu {X_temp[col].dtype} kh√¥ng th·ªÉ chuy·ªÉn ƒë·ªïi th√†nh d·∫°ng s·ªë.")

                 return None, None, None, None
    
    try:
        X_processed_features = X_processed_features.astype(float)
    except Exception as e:
        if st_instance: st_instance.error(f"L·ªói khi √©p ki·ªÉu t·∫•t c·∫£ c√°c c·ªôt thu·ªôc t√≠nh sang float: {e}")
        return None, None, None, None

    return X_processed_features, y_processed_nb, le_y_for_nb, feature_label_encoders


def train_naive_bayes_model(X_train_data, y_train_data, nb_method_type='gaussian', alpha_val=1.0, st_instance=None):
    model = None
    X_train_data_copy = X_train_data.copy() 

    if nb_method_type == 'gaussian':
        model = GaussianNB()
    elif nb_method_type == 'multinomial':
        if (X_train_data_copy < 0).any().any():
            if st_instance: st_instance.warning("‚ö†Ô∏è D·ªØ li·ªáu c√≥ gi√° tr·ªã √¢m. MultinomialNB y√™u c·∫ßu c√°c ƒë·∫∑c tr∆∞ng kh√¥ng √¢m.")
            raise ValueError("MultinomialNB kh√¥ng th·ªÉ x·ª≠ l√Ω gi√° tr·ªã √¢m. C·∫ßn chu·∫©n h√≥a tr∆∞·ªõc.")
        model = MultinomialNB(alpha=alpha_val)
    else:
        if st_instance: st_instance.error("Ph∆∞∆°ng ph√°p Naive Bayes kh√¥ng h·ª£p l·ªá.")
        raise ValueError("Ph∆∞∆°ng ph√°p Naive Bayes kh√¥ng h·ª£p l·ªá.")

    try:
        model.fit(X_train_data_copy, y_train_data)
        accuracy = model.score(X_train_data_copy, y_train_data)
        return model, accuracy
    except Exception as e:
        if st_instance: st_instance.error(f"L·ªói khi hu·∫•n luy·ªán m√¥ h√¨nh Naive Bayes ({nb_method_type}): {e}")
        raise e 


def run_kmeans_clustering_analysis(X_data_for_kmeans, num_k_clusters=3):
    if X_data_for_kmeans.shape[1] < 1:
        raise ValueError("D·ªØ li·ªáu c·∫ßn √≠t nh·∫•t 1 thu·ªôc t√≠nh ƒë·ªÉ ch·∫°y K-Means.")
    
    scaler_kmeans = MinMaxScaler()
    X_scaled_for_kmeans = scaler_kmeans.fit_transform(X_data_for_kmeans)
    
    kmeans_model = KMeans(n_clusters=num_k_clusters, random_state=42, n_init="auto")
    cluster_labels = kmeans_model.fit_predict(X_scaled_for_kmeans)
    cluster_centers = kmeans_model.cluster_centers_

    df_clustered_results = X_data_for_kmeans.copy()
    df_clustered_results["Cluster"] = cluster_labels
    
    return cluster_labels, cluster_centers, df_clustered_results, X_scaled_for_kmeans


def train_kohonen_som_model(X_data_for_som, som_rows, som_cols, sigma_val=1.0, lr_val=0.5, iterations_val=100, st_instance=None):
    if X_data_for_som.shape[0] == 0 or X_data_for_som.shape[1] == 0:
        raise ValueError("D·ªØ li·ªáu r·ªóng ho·∫∑c kh√¥ng c√≥ thu·ªôc t√≠nh. Kh√¥ng th·ªÉ hu·∫•n luy·ªán SOM.")
    
    scaler_som = MinMaxScaler()
    X_scaled_for_som = scaler_som.fit_transform(X_data_for_som)
    input_length = X_scaled_for_som.shape[1]
    
    som_instance = MiniSom(
        x=som_cols, y=som_rows, input_len=input_length, 
        sigma=sigma_val, learning_rate=lr_val, random_seed=42 
    )
    som_instance.random_weights_init(X_scaled_for_som)
    if st_instance: st_instance.write(f"ƒêang hu·∫•n luy·ªán SOM v·ªõi {X_scaled_for_som.shape[0]} ƒëi·ªÉm, {iterations_val} v√≤ng l·∫∑p...")
    som_instance.train_random(X_scaled_for_som, iterations_val) 
    if st_instance: st_instance.success("‚úÖ Hu·∫•n luy·ªán SOM ho√†n t·∫•t!")
    return som_instance, X_scaled_for_som

# ===== DETAILED CALCULATION EXPLANATIONS =====

def get_apriori_detailed_explanation(df, min_support=0.4, min_confidence=0.6):
    """
    Tr·∫£ v·ªÅ gi·∫£i th√≠ch chi ti·∫øt t·ª´ng b∆∞·ªõc t√≠nh to√°n Apriori
    """
    explanations = []
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu
    df_encoded = pd.get_dummies(df.iloc[:, 1], prefix='', prefix_sep='')
    total_transactions = len(df_encoded)
    
    explanations.append(f"**üìä D·ªØ li·ªáu ƒë·∫ßu v√†o:**")
    explanations.append(f"- T·ªïng s·ªë giao d·ªãch: {total_transactions}")
    explanations.append(f"- Min Support: {min_support} ({min_support*100}%)")
    explanations.append(f"- Min Confidence: {min_confidence} ({min_confidence*100}%)")
    explanations.append("")
    
    # T√≠nh support cho t·ª´ng item
    explanations.append(f"**üîç B∆∞·ªõc 1: T√≠nh Support cho t·ª´ng item**")
    item_support = {}
    for col in df_encoded.columns:
        count = df_encoded[col].sum()
        support = count / total_transactions
        item_support[col] = support
        
        if support >= min_support:
            status = "‚úÖ ƒê·∫†T"
        else:
            status = "‚ùå LO·∫†I"
        
        explanations.append(f"- Item '{col}': {count}/{total_transactions} = {support:.3f} {status}")
    
    # L1 - Frequent 1-itemsets
    L1 = {item: support for item, support in item_support.items() if support >= min_support}
    explanations.append("")
    explanations.append(f"**üìã L1 (Frequent 1-itemsets): {len(L1)} items**")
    for item, support in L1.items():
        explanations.append(f"- {{{item}}}: {support:.3f}")
    
    # Generate C2 candidates
    explanations.append("")
    explanations.append(f"**üîç B∆∞·ªõc 2: T·∫°o C2 candidates v√† t√≠nh Support**")
    
    items = list(L1.keys())
    pair_support = {}
    
    for i in range(len(items)):
        for j in range(i+1, len(items)):
            item1, item2 = items[i], items[j]
            # T√≠nh support cho c·∫∑p
            count = ((df_encoded[item1] == 1) & (df_encoded[item2] == 1)).sum()
            support = count / total_transactions
            pair_support[(item1, item2)] = support
            
            if support >= min_support:
                status = "‚úÖ ƒê·∫†T"
            else:
                status = "‚ùå LO·∫†I"
            
            explanations.append(f"- {{{item1}, {item2}}}: {count}/{total_transactions} = {support:.3f} {status}")
    
    # L2 - Frequent 2-itemsets
    L2 = {pair: support for pair, support in pair_support.items() if support >= min_support}
    explanations.append("")
    explanations.append(f"**üìã L2 (Frequent 2-itemsets): {len(L2)} pairs**")
    for pair, support in L2.items():
        explanations.append(f"- {{{pair[0]}, {pair[1]}}}: {support:.3f}")
    
    # T√≠nh Association Rules
    explanations.append("")
    explanations.append(f"**üîç B∆∞·ªõc 3: T·∫°o Association Rules**")
    
    rules = []
    for pair, pair_support in L2.items():
        item1, item2 = pair
        
        # Rule: item1 -> item2
        conf1 = pair_support / L1[item1]
        if conf1 >= min_confidence:
            rules.append((item1, item2, conf1))
            status1 = "‚úÖ M·∫†NH"
        else:
            status1 = "‚ùå Y·∫æU"
        
        explanations.append(f"- {item1} ‚Üí {item2}: {pair_support:.3f}/{L1[item1]:.3f} = {conf1:.3f} {status1}")
        
        # Rule: item2 -> item1  
        conf2 = pair_support / L1[item2]
        if conf2 >= min_confidence:
            rules.append((item2, item1, conf2))
            status2 = "‚úÖ M·∫†NH"
        else:
            status2 = "‚ùå Y·∫æU"
        
        explanations.append(f"- {item2} ‚Üí {item1}: {pair_support:.3f}/{L1[item2]:.3f} = {conf2:.3f} {status2}")
    
    # T·ªïng k·∫øt
    explanations.append("")
    explanations.append(f"**üìä T·ªïng k·∫øt k·∫øt qu·∫£:**")
    explanations.append(f"- Frequent 1-itemsets: {len(L1)}")
    explanations.append(f"- Frequent 2-itemsets: {len(L2)}")
    explanations.append(f"- Strong Association Rules: {len(rules)}")
    
    return "\n".join(explanations)

def get_rough_set_detailed_explanation(df, target_class, condition_attrs, decision_attr):
    """
    Tr·∫£ v·ªÅ gi·∫£i th√≠ch chi ti·∫øt t·ª´ng b∆∞·ªõc t√≠nh to√°n Rough Set
    """
    explanations = []
    
    # Th√¥ng tin c∆° b·∫£n
    total_objects = len(df)
    target_objects = df[df[decision_attr] == target_class].index.tolist()
    
    explanations.append(f"**üìä D·ªØ li·ªáu ƒë·∫ßu v√†o:**")
    explanations.append(f"- T·ªïng s·ªë ƒë·ªëi t∆∞·ª£ng: {total_objects}")
    explanations.append(f"- Thu·ªôc t√≠nh ƒëi·ªÅu ki·ªán: {', '.join(condition_attrs)}")
    explanations.append(f"- Thu·ªôc t√≠nh quy·∫øt ƒë·ªãnh: {decision_attr}")
    explanations.append(f"- L·ªõp m·ª•c ti√™u: {target_class}")
    explanations.append(f"- S·ªë ƒë·ªëi t∆∞·ª£ng thu·ªôc l·ªõp '{target_class}': {len(target_objects)}")
    explanations.append("")
    
    # T·∫°o equivalence classes
    explanations.append(f"**üîç B∆∞·ªõc 1: T·∫°o Equivalence Classes**")
    equivalence_classes = {}
    
    for idx, row in df.iterrows():
        # T·∫°o key t·ª´ c√°c thu·ªôc t√≠nh ƒëi·ªÅu ki·ªán
        key = tuple(row[attr] for attr in condition_attrs)
        if key not in equivalence_classes:
            equivalence_classes[key] = []
        equivalence_classes[key].append(idx)
    
    for i, (key, objects) in enumerate(equivalence_classes.items(), 1):
        key_str = ', '.join([f"{condition_attrs[j]}={key[j]}" for j in range(len(key))])
        explanations.append(f"- L·ªõp t∆∞∆°ng ƒë∆∞∆°ng {i}: [{key_str}] = {{{', '.join(map(str, objects))}}} ({len(objects)} objects)")
    
    explanations.append("")
    
    # T√≠nh Lower Approximation
    explanations.append(f"**üîç B∆∞·ªõc 2: T√≠nh Lower Approximation R(X)**")
    lower_approx = []
    
    for key, eq_class in equivalence_classes.items():
        # Ki·ªÉm tra t·∫•t c·∫£ objects trong equivalence class c√≥ thu·ªôc target class kh√¥ng
        all_in_target = all(df.loc[obj, decision_attr] == target_class for obj in eq_class)
        
        key_str = ', '.join([f"{condition_attrs[j]}={key[j]}" for j in range(len(key))])
        
        if all_in_target:
            lower_approx.extend(eq_class)
            explanations.append(f"- L·ªõp [{key_str}]: T·∫§T C·∫¢ thu·ªôc '{target_class}' ‚Üí ‚úÖ TH√äM V√ÄO R(X)")
        else:
            explanations.append(f"- L·ªõp [{key_str}]: KH√îNG ph·∫£i t·∫•t c·∫£ thu·ªôc '{target_class}' ‚Üí ‚ùå LO·∫†I")
    
    explanations.append(f"- **Lower Approximation R(X) = {{{', '.join(map(str, sorted(lower_approx)))}}} ({len(lower_approx)} objects)**")
    explanations.append("")
    
    # T√≠nh Upper Approximation
    explanations.append(f"**üîç B∆∞·ªõc 3: T√≠nh Upper Approximation RÃÑ(X)**")
    upper_approx = []
    
    for key, eq_class in equivalence_classes.items():
        # Ki·ªÉm tra c√≥ √≠t nh·∫•t 1 object thu·ªôc target class kh√¥ng
        any_in_target = any(df.loc[obj, decision_attr] == target_class for obj in eq_class)
        
        key_str = ', '.join([f"{condition_attrs[j]}={key[j]}" for j in range(len(key))])
        
        if any_in_target:
            upper_approx.extend(eq_class)
            explanations.append(f"- L·ªõp [{key_str}]: C√ì √çT NH·∫§T 1 thu·ªôc '{target_class}' ‚Üí ‚úÖ TH√äM V√ÄO RÃÑ(X)")
        else:
            explanations.append(f"- L·ªõp [{key_str}]: KH√îNG c√≥ object n√†o thu·ªôc '{target_class}' ‚Üí ‚ùå LO·∫†I")
    
    explanations.append(f"- **Upper Approximation RÃÑ(X) = {{{', '.join(map(str, sorted(upper_approx)))}}} ({len(upper_approx)} objects)**")
    explanations.append("")
    
    # T√≠nh Accuracy
    accuracy = len(lower_approx) / len(upper_approx) if upper_approx else 0
    explanations.append(f"**üîç B∆∞·ªõc 4: T√≠nh Accuracy**")
    explanations.append(f"- Accuracy Œ±(X) = |R(X)| / |RÃÑ(X)| = {len(lower_approx)} / {len(upper_approx)} = {accuracy:.3f}")
    
    if accuracy == 1.0:
        explanations.append(f"- **Accuracy = 1.0**: T·∫≠p X ho√†n to√†n x√°c ƒë·ªãnh (crisp set)")
    elif accuracy == 0.0:
        explanations.append(f"- **Accuracy = 0.0**: T·∫≠p X ho√†n to√†n kh√¥ng x√°c ƒë·ªãnh")
    else:
        explanations.append(f"- **Accuracy = {accuracy:.3f}**: T·∫≠p X m·ªôt ph·∫ßn x√°c ƒë·ªãnh (rough set)")
    
    explanations.append("")
    
    # T√≠nh Dependency
    total_lower = len(lower_approx)
    total_universe = len(df)
    dependency = total_lower / total_universe
    
    explanations.append(f"**üîç B∆∞·ªõc 5: T√≠nh Dependency**")
    explanations.append(f"- Dependency Œ≥ = |R(X)| / |U| = {total_lower} / {total_universe} = {dependency:.3f}")
    explanations.append(f"- **Gi·∫£i th√≠ch**: {dependency*100:.1f}% ƒë·ªëi t∆∞·ª£ng c√≥ th·ªÉ ph√¢n lo·∫°i ch·∫Øc ch·∫Øn")
    
    return "\n".join(explanations)

def get_decision_tree_detailed_explanation(df, selected_features, target_attr, method='Gain'):
    """
    Tr·∫£ v·ªÅ gi·∫£i th√≠ch chi ti·∫øt t·ª´ng b∆∞·ªõc t√≠nh to√°n Decision Tree
    """
    explanations = []
    
    # Th√¥ng tin c∆° b·∫£n
    total_samples = len(df)
    class_counts = df[target_attr].value_counts()
    
    # Debug: Ki·ªÉm tra type c·ªßa class_counts
    print(f"DEBUG: class_counts type: {type(class_counts)}")
    print(f"DEBUG: class_counts: {class_counts}")
    
    explanations.append(f"**üìä D·ªØ li·ªáu ƒë·∫ßu v√†o:**")
    explanations.append(f"- T·ªïng s·ªë samples: {total_samples}")
    explanations.append(f"- S·ªë features: {len(selected_features)}")
    explanations.append(f"- Target attribute: {target_attr}")
    explanations.append(f"- Method: {method}")
    explanations.append("")
    
    explanations.append(f"**üìà Ph√¢n b·ªë classes:**")
    for cls, count in class_counts.items():
        proportion = count / total_samples
        explanations.append(f"- {cls}: {count}/{total_samples} = {proportion:.3f}")
    explanations.append("")
    
    # T√≠nh Entropy c·ªßa t·∫≠p g·ªëc
    import math
    entropy_total = 0
    # X·ª≠ l√Ω an to√†n cho class_counts
    try:
        if hasattr(class_counts, 'values') and callable(getattr(class_counts, 'values', None)):
            count_values = class_counts.values()
        elif hasattr(class_counts, 'values'):
            count_values = class_counts.values
        else:
            count_values = class_counts
        
        for count in count_values:
            if count > 0:
                p = count / total_samples
                entropy_total -= p * math.log2(p)
    except Exception as e:
        print(f"DEBUG: Error in entropy calculation: {e}")
        # Fallback: t√≠nh entropy th·ªß c√¥ng
        for cls, count in class_counts.items():
            if count > 0:
                p = count / total_samples
                entropy_total -= p * math.log2(p)
    
    explanations.append(f"**üîç B∆∞·ªõc 1: T√≠nh Entropy c·ªßa t·∫≠p g·ªëc**")
    calculation_parts = []
    for cls, count in class_counts.items():
        if count > 0:
            p = count / total_samples
            part = f"({p:.3f} √ó log‚ÇÇ({p:.3f}))"
            calculation_parts.append(part)
    
    explanations.append(f"- E(S) = -Œ£·µ¢ p·µ¢ √ó log‚ÇÇ(p·µ¢)")
    explanations.append(f"- E(S) = -[{' + '.join(calculation_parts)}]")
    explanations.append(f"- **E(S) = {entropy_total:.4f}**")
    explanations.append("")
    
    # T√≠nh Information Gain cho m·ªói feature
    explanations.append(f"**üîç B∆∞·ªõc 2: T√≠nh Information Gain cho m·ªói feature**")
    
    feature_gains = {}
    for feature in selected_features:
        feature_values = df[feature].unique()
        weighted_entropy = 0
        
        explanations.append(f"**Feature: {feature}**")
        
        for value in feature_values:
            subset = df[df[feature] == value]
            subset_size = len(subset)
            weight = subset_size / total_samples
            
            # T√≠nh entropy c·ªßa subset
            subset_class_counts = subset[target_attr].value_counts()
            subset_entropy = 0
            
            entropy_parts = []
            for cls, count in subset_class_counts.items():
                if count > 0:
                    p = count / subset_size
                    subset_entropy -= p * math.log2(p)
                    entropy_parts.append(f"({p:.3f} √ó log‚ÇÇ({p:.3f}))")
            
            weighted_entropy += weight * subset_entropy
            
            explanations.append(f"  - {feature}={value}: {subset_size} samples")
            explanations.append(f"    Classes: {dict(subset_class_counts)}")
            explanations.append(f"    E(S_{value}) = -[{' + '.join(entropy_parts)}] = {subset_entropy:.4f}")
        
        information_gain = entropy_total - weighted_entropy
        feature_gains[feature] = information_gain
        
        explanations.append(f"  - **IG({feature}) = {entropy_total:.4f} - {weighted_entropy:.4f} = {information_gain:.4f}**")
        explanations.append("")
    
    # Ch·ªçn feature t·ªët nh·∫•t
    best_feature = max(feature_gains, key=feature_gains.get)
    best_gain = feature_gains[best_feature]
    
    explanations.append(f"**üîç B∆∞·ªõc 3: Ch·ªçn Root Node**")
    explanations.append(f"- Feature c√≥ Information Gain cao nh·∫•t: **{best_feature}** (IG = {best_gain:.4f})")
    explanations.append(f"- **{best_feature}** ƒë∆∞·ª£c ch·ªçn l√†m Root Node")
    explanations.append("")
    
    # Ranking c√°c features
    sorted_features = sorted(feature_gains.items(), key=lambda x: x[1], reverse=True)
    explanations.append(f"**üìä Ranking Information Gain:**")
    for i, (feat, gain) in enumerate(sorted_features, 1):
        explanations.append(f"{i}. {feat}: {gain:.4f}")
    
    return "\n".join(explanations)

def get_naive_bayes_detailed_explanation(X, y, nb_type='gaussian'):
    """
    Tr·∫£ v·ªÅ gi·∫£i th√≠ch chi ti·∫øt t·ª´ng b∆∞·ªõc t√≠nh to√°n Naive Bayes
    """
    explanations = []
    
    # Th√¥ng tin c∆° b·∫£n
    n_samples, n_features = X.shape
    classes = np.unique(y)
    
    explanations.append(f"**üìä D·ªØ li·ªáu ƒë·∫ßu v√†o:**")
    explanations.append(f"- S·ªë samples: {n_samples}")
    explanations.append(f"- S·ªë features: {n_features}")
    explanations.append(f"- S·ªë classes: {len(classes)}")
    explanations.append(f"- Classes: {list(classes)}")
    explanations.append(f"- Naive Bayes type: {nb_type}")
    explanations.append("")
    
    # T√¨nh Prior Probabilities
    explanations.append(f"**üîç B∆∞·ªõc 1: T√≠nh Prior Probabilities P(y)**")
    class_counts = pd.Series(y).value_counts()
    
    for cls in classes:
        count = class_counts.get(cls, 0)
        prior = count / n_samples
        explanations.append(f"- P(y={cls}) = {count}/{n_samples} = {prior:.4f}")
    explanations.append("")
    
    # T√≠nh Likelihood probabilities cho m·ªói feature
    explanations.append(f"**üîç B∆∞·ªõc 2: T√≠nh Likelihood P(x·µ¢|y) cho m·ªói feature**")
    
    if isinstance(X, pd.DataFrame):
        feature_names = X.columns.tolist()
    else:
        feature_names = [f"Feature_{i}" for i in range(n_features)]
    
    for i, feature_name in enumerate(feature_names[:3]):  # Ch·ªâ hi·ªÉn th·ªã 3 features ƒë·∫ßu
        explanations.append(f"**Feature: {feature_name}**")
        
        if nb_type == 'gaussian':
            # Gaussian NB - t√≠nh mean v√† std cho m·ªói class
            for cls in classes:
                mask = (y == cls)
                feature_values = X.iloc[:, i] if isinstance(X, pd.DataFrame) else X[:, i]
                cls_values = feature_values[mask]
                
                mean_val = np.mean(cls_values)
                std_val = np.std(cls_values)
                
                explanations.append(f"  - Class {cls}: Œº={mean_val:.4f}, œÉ={std_val:.4f}")
        
        else:  # Multinomial NB
            # T√≠nh t·∫ßn su·∫•t cho m·ªói gi√° tr·ªã
            unique_values = np.unique(X.iloc[:, i] if isinstance(X, pd.DataFrame) else X[:, i])
            
            for cls in classes:
                mask = (y == cls)
                feature_values = X.iloc[:, i] if isinstance(X, pd.DataFrame) else X[:, i]
                cls_values = feature_values[mask]
                
                explanations.append(f"  - Class {cls}:")
                for val in unique_values[:3]:  # Ch·ªâ hi·ªÉn th·ªã 3 gi√° tr·ªã ƒë·∫ßu
                    count = np.sum(cls_values == val)
                    total = len(cls_values)
                    prob = count / total
                    explanations.append(f"    P({feature_name}={val}|{cls}) = {count}/{total} = {prob:.4f}")
        
        explanations.append("")
    
    # V√≠ d·ª• d·ª± ƒëo√°n
    explanations.append(f"**üîç B∆∞·ªõc 3: V√≠ d·ª• d·ª± ƒëo√°n cho sample ƒë·∫ßu ti√™n**")
    first_sample = X.iloc[0] if isinstance(X, pd.DataFrame) else X[0]
    true_label = y[0]
    
    explanations.append(f"- Sample: {list(first_sample)}")
    explanations.append(f"- True label: {true_label}")
    explanations.append("")
    
    # T√≠nh posterior cho m·ªói class
    for cls in classes:
        prior = class_counts.get(cls, 0) / n_samples
        explanations.append(f"**Class {cls}:**")
        explanations.append(f"- Prior P({cls}) = {prior:.4f}")
        
        # Simplified likelihood calculation
        explanations.append(f"- Likelihood P(X|{cls}) = ‚àè·µ¢ P(x·µ¢|{cls}) [t√≠nh t·ª´ model]")
        explanations.append(f"- Posterior ‚àù P({cls}) √ó P(X|{cls})")
        explanations.append("")
    
    explanations.append(f"**üìä K·∫øt qu·∫£ d·ª± ƒëo√°n:**")
    explanations.append(f"- Ch·ªçn class c√≥ posterior probability cao nh·∫•t")
    explanations.append(f"- √Åp d·ª•ng Laplace smoothing ƒë·ªÉ tr√°nh P=0")
    
    return "\n".join(explanations)

def get_kmeans_detailed_explanation(X, n_clusters=3, random_state=42):
    """
    Tr·∫£ v·ªÅ gi·∫£i th√≠ch chi ti·∫øt t·ª´ng b∆∞·ªõc t√≠nh to√°n K-Means
    """
    explanations = []
    
    # Th√¥ng tin c∆° b·∫£n
    n_samples, n_features = X.shape
    
    explanations.append(f"**üìä D·ªØ li·ªáu ƒë·∫ßu v√†o:**")
    explanations.append(f"- S·ªë samples: {n_samples}")
    explanations.append(f"- S·ªë features: {n_features}")
    explanations.append(f"- S·ªë clusters (K): {n_clusters}")
    explanations.append(f"- Random state: {random_state}")
    explanations.append("")
    
    # Kh·ªüi t·∫°o centroids
    np.random.seed(random_state)
    initial_centroids = []
    
    if isinstance(X, pd.DataFrame):
        X_array = X.values
        feature_names = X.columns.tolist()
    else:
        X_array = X
        feature_names = [f"Feature_{i}" for i in range(n_features)]
    
    # Random initialization c·ªßa centroids
    for k in range(n_clusters):
        centroid = []
        for j in range(n_features):
            min_val = X_array[:, j].min()
            max_val = X_array[:, j].max()
            random_val = np.random.uniform(min_val, max_val)
            centroid.append(random_val)
        initial_centroids.append(centroid)
    
    explanations.append(f"**üîç B∆∞·ªõc 1: Kh·ªüi t·∫°o K centroids ng·∫´u nhi√™n**")
    for k, centroid in enumerate(initial_centroids):
        centroid_str = ', '.join([f"{feature_names[j]}={val:.3f}" for j, val in enumerate(centroid)])
        explanations.append(f"- Centroid {k+1}: ({centroid_str})")
    explanations.append("")
    
    # Simulation c·ªßa v√†i iterations ƒë·∫ßu
    explanations.append(f"**üîç B∆∞·ªõc 2: G√°n samples v√†o clusters (Iteration 1)**")
    
    # T√≠nh kho·∫£ng c√°ch t·ª´ m·ªói sample ƒë·∫øn m·ªói centroid
    distances = np.zeros((n_samples, n_clusters))
    for i in range(n_samples):
        for k in range(n_clusters):
            # Euclidean distance
            dist = np.sqrt(np.sum((X_array[i] - initial_centroids[k])**2))
            distances[i, k] = dist
    
    # G√°n cluster cho m·ªói sample
    cluster_assignments = np.argmin(distances, axis=1)
    
    # Hi·ªÉn th·ªã v√†i samples ƒë·∫ßu
    for i in range(min(5, n_samples)):
        sample_str = ', '.join([f"{val:.3f}" for val in X_array[i]])
        dist_str = ', '.join([f"{dist:.3f}" for dist in distances[i]])
        assigned_cluster = cluster_assignments[i] + 1
        
        explanations.append(f"- Sample {i+1}: ({sample_str})")
        explanations.append(f"  Distances: [{dist_str}]")
        explanations.append(f"  ‚Üí Assigned to Cluster {assigned_cluster}")
    
    if n_samples > 5:
        explanations.append(f"  ... (v√† {n_samples-5} samples kh√°c)")
    explanations.append("")
    
    # C·∫≠p nh·∫≠t centroids
    explanations.append(f"**üîç B∆∞·ªõc 3: C·∫≠p nh·∫≠t centroids**")
    new_centroids = []
    
    for k in range(n_clusters):
        cluster_samples = X_array[cluster_assignments == k]
        if len(cluster_samples) > 0:
            new_centroid = np.mean(cluster_samples, axis=0)
            new_centroids.append(new_centroid)
            
            centroid_str = ', '.join([f"{feature_names[j]}={val:.3f}" for j, val in enumerate(new_centroid)])
            explanations.append(f"- Cluster {k+1}: {len(cluster_samples)} samples")
            explanations.append(f"  New centroid: ({centroid_str})")
        else:
            new_centroids.append(initial_centroids[k])
            explanations.append(f"- Cluster {k+1}: 0 samples (gi·ªØ nguy√™n centroid)")
    
    explanations.append("")
    
    # T√≠nh cost function (WCSS)
    wcss = 0
    for i in range(n_samples):
        assigned_cluster = cluster_assignments[i]
        centroid = new_centroids[assigned_cluster]
        wcss += np.sum((X_array[i] - centroid)**2)
    
    explanations.append(f"**üîç B∆∞·ªõc 4: T√≠nh Cost Function (WCSS)**")
    explanations.append(f"- WCSS = Œ£·µ¢ ||x·µ¢ - Œº‚Çñ||¬≤ = {wcss:.4f}")
    explanations.append(f"- **M·ª•c ti√™u**: Minimize WCSS qua c√°c iterations")
    explanations.append("")
    
    # ƒêi·ªÅu ki·ªán h·ªôi t·ª•
    explanations.append(f"**üîç B∆∞·ªõc 5: Ki·ªÉm tra h·ªôi t·ª•**")
    explanations.append(f"- So s√°nh centroids m·ªõi vs c≈©")
    explanations.append(f"- D·ª´ng khi: centroids kh√¥ng ƒë·ªïi ho·∫∑c thay ƒë·ªïi < threshold")
    explanations.append(f"- Ho·∫∑c ƒë·∫°t max_iterations")
    explanations.append("")
    
    # T·ªïng k·∫øt
    cluster_sizes = np.bincount(cluster_assignments)
    explanations.append(f"**üìä T·ªïng k·∫øt (sau iteration 1):**")
    for k in range(n_clusters):
        size = cluster_sizes[k] if k < len(cluster_sizes) else 0
        explanations.append(f"- Cluster {k+1}: {size} samples")
    explanations.append(f"- WCSS: {wcss:.4f}")
    
    return "\n".join(explanations)

def get_som_detailed_explanation(X, grid_size=(2, 2), learning_rate=0.1, epochs=100):
    """
    Tr·∫£ v·ªÅ gi·∫£i th√≠ch chi ti·∫øt t·ª´ng b∆∞·ªõc t√≠nh to√°n SOM
    """
    explanations = []
    
    # Th√¥ng tin c∆° b·∫£n
    n_samples, n_features = X.shape
    
    explanations.append(f"**üìä D·ªØ li·ªáu ƒë·∫ßu v√†o:**")
    explanations.append(f"- S·ªë samples: {n_samples}")
    explanations.append(f"- S·ªë features: {n_features}")
    explanations.append(f"- Grid size: {grid_size[0]}√ó{grid_size[1]} = {grid_size[0]*grid_size[1]} neurons")
    explanations.append(f"- Learning rate Œ±‚ÇÄ: {learning_rate}")
    explanations.append(f"- Epochs: {epochs}")
    explanations.append("")
    
    # Kh·ªüi t·∫°o weight matrix
    n_neurons = grid_size[0] * grid_size[1]
    np.random.seed(42)
    weights = np.random.rand(n_neurons, n_features)
    
    explanations.append(f"**üîç B∆∞·ªõc 1: Kh·ªüi t·∫°o Weight Matrix**")
    explanations.append(f"- M·ªói neuron c√≥ {n_features} weights (t∆∞∆°ng ·ª©ng v·ªõi s·ªë features)")
    explanations.append(f"- Kh·ªüi t·∫°o ng·∫´u nhi√™n trong [0, 1]")
    explanations.append("")
    
    # Hi·ªÉn th·ªã weights ban ƒë·∫ßu
    if isinstance(X, pd.DataFrame):
        feature_names = X.columns.tolist()
        X_array = X.values
    else:
        feature_names = [f"Feature_{i}" for i in range(n_features)]
        X_array = X
    
    for i in range(min(4, n_neurons)):
        weight_str = ', '.join([f"{feature_names[j]}={weights[i,j]:.3f}" for j in range(n_features)])
        row, col = divmod(i, grid_size[1])
        explanations.append(f"- Neuron ({row},{col}): W = [{weight_str}]")
    
    if n_neurons > 4:
        explanations.append(f"  ... (v√† {n_neurons-4} neurons kh√°c)")
    explanations.append("")
    
    # Simulation c·ªßa m·ªôt epoch
    explanations.append(f"**üîç B∆∞·ªõc 2: Training Process (Epoch 1)**")
    
    # Ch·ªçn sample ƒë·∫ßu ti√™n ƒë·ªÉ demo
    sample_idx = 0
    input_vector = X_array[sample_idx]
    sample_str = ', '.join([f"{val:.3f}" for val in input_vector])
    
    explanations.append(f"**Sample {sample_idx+1}: [{sample_str}]**")
    explanations.append("")
    
    # T√≠nh kho·∫£ng c√°ch ƒë·∫øn t·∫•t c·∫£ neurons
    distances = []
    for i in range(n_neurons):
        dist = np.sqrt(np.sum((input_vector - weights[i])**2))
        distances.append(dist)
        
        row, col = divmod(i, grid_size[1])
        explanations.append(f"- Distance to Neuron ({row},{col}): {dist:.4f}")
    
    # T√¨m BMU (Best Matching Unit)
    bmu_idx = np.argmin(distances)
    bmu_row, bmu_col = divmod(bmu_idx, grid_size[1])
    explanations.append("")
    explanations.append(f"**BMU (Best Matching Unit): Neuron ({bmu_row},{bmu_col})** (distance = {distances[bmu_idx]:.4f})")
    explanations.append("")
    
    # C·∫≠p nh·∫≠t weights
    explanations.append(f"**üîç B∆∞·ªõc 3: C·∫≠p nh·∫≠t Weights**")
    explanations.append(f"- C√¥ng th·ª©c: W_new = W_old + Œ±(t) √ó h(t) √ó (X - W_old)")
    explanations.append(f"- Œ±(t) = learning rate t·∫°i th·ªùi ƒëi·ªÉm t")
    explanations.append(f"- h(t) = neighborhood function")
    explanations.append("")
    
    # C·∫≠p nh·∫≠t BMU v√† neighbors
    for i in range(n_neurons):
        row, col = divmod(i, grid_size[1])
        
        # T√≠nh neighborhood distance
        neighborhood_dist = np.sqrt((row - bmu_row)**2 + (col - bmu_col)**2)
        
        # Neighborhood function (Gaussian)
        sigma = 1.0  # neighborhood radius
        neighborhood_influence = np.exp(-(neighborhood_dist**2) / (2 * sigma**2))
        
        # Weight update
        old_weights = weights[i].copy()
        weights[i] += learning_rate * neighborhood_influence * (input_vector - weights[i])
        
        if i == bmu_idx:
            explanations.append(f"- BMU ({row},{col}): h={neighborhood_influence:.3f}")
            weight_change = weights[i] - old_weights
            change_str = ', '.join([f"{val:+.4f}" for val in weight_change])
            explanations.append(f"  Weight change: [{change_str}]")
        elif neighborhood_influence > 0.1:  # Ch·ªâ hi·ªÉn th·ªã neighbors c√≥ ·∫£nh h∆∞·ªüng ƒë√°ng k·ªÉ
            explanations.append(f"- Neighbor ({row},{col}): h={neighborhood_influence:.3f}")
    
    explanations.append("")
    
    # Convergence v√† learning schedule
    explanations.append(f"**üîç B∆∞·ªõc 4: Learning Schedule**")
    explanations.append(f"- Learning rate decay: Œ±(t) = Œ±‚ÇÄ √ó exp(-t/œÑ)")
    explanations.append(f"- Neighborhood decay: œÉ(t) = œÉ‚ÇÄ √ó exp(-t/œÑ)")
    explanations.append(f"- Qua {epochs} epochs, Œ± v√† œÉ gi·∫£m d·∫ßn")
    explanations.append("")
    
    # Final mapping
    explanations.append(f"**üìä K·∫øt qu·∫£ cu·ªëi c√πng:**")
    explanations.append(f"- M·ªói sample ƒë∆∞·ª£c map v√†o neuron g·∫ßn nh·∫•t")
    explanations.append(f"- C√°c samples t∆∞∆°ng t·ª± s·∫Ω activate c√πng neuron ho·∫∑c neurons g·∫ßn nhau")
    explanations.append(f"- T·∫°o ra topological map 2D c·ªßa d·ªØ li·ªáu high-dimensional")
    
    return "\n".join(explanations)
