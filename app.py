import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import math # C·∫ßn cho math.ceil trong SOM plot layout
from sklearn.preprocessing import MinMaxScaler

# Import c√°c h√†m t·ª´ func.py
import func 

# C·∫•u h√¨nh trang Streamlit
st.set_page_config(layout="wide", page_title="ƒê·ªì √°n cu·ªëi k·ª≥: Khai th√°c D·ªØ li·ªáu")


# ==============================================================================
# GIAO DI·ªÜN STREAMLIT CH√çNH
# ==============================================================================
st.title("ƒê·ªì √°n cu·ªëi k·ª≥: Khai th√°c D·ªØ li·ªáu")
st.markdown("""
**Nh√≥m sinh vi√™n th·ª±c hi·ªán:**
1. Tr·∫ßn Nh·∫≠t Kh√°nh
2. Tr·∫ßn Nh·∫≠t Huy
""")

# --- T·∫£i d·ªØ li·ªáu ---
st.sidebar.header("üìÅ 1. T·∫£i D·ªØ Li·ªáu L√™n")
uploaded_file = st.sidebar.file_uploader("Ch·ªçn m·ªôt t·ªáp CSV", type=["csv"])

df_original = None
if uploaded_file is not None:
    try:
        df_original = pd.read_csv(uploaded_file)
        st.subheader("üìÑ D·ªØ li·ªáu g·ªëc (5 d√≤ng ƒë·∫ßu)")
        st.dataframe(df_original.head())
    except Exception as e:
        st.error(f"L·ªói khi ƒë·ªçc t·ªáp CSV: {e}")
        df_original = None

# --- Ch·ªçn thu·∫≠t to√°n ---
st.sidebar.header("‚öôÔ∏è 2. Ch·ªçn Thu·∫≠t To√°n Ph√¢n T√≠ch")
analysis_options = [
    "--- Ch·ªçn m·ªôt thu·∫≠t to√°n ---",
    "Apriori (Lu·∫≠t K·∫øt H·ª£p)",
    "Rough Set (L√Ω Thuy·∫øt T·∫≠p Th√¥)",
    "Decision Tree (ID3)",
    "Naive Bayes",
    "K-Means Clustering",
    "Kohonen SOM"
]
selected_analysis = st.sidebar.selectbox("Ch·ªçn m·ªôt ph∆∞∆°ng ph√°p:", analysis_options, key="main_analysis_choice")

if df_original is not None and selected_analysis != analysis_options[0]:
    st.markdown("---")
    st.header(f" Ph√¢n t√≠ch b·∫±ng: {selected_analysis.split('(')[0].strip()}")

    # APRIORI
    if selected_analysis == analysis_options[1]:
        
        df_apriori_input = df_original.copy()
        df_apriori_input = df_apriori_input.loc[:, ~df_apriori_input.columns.str.startswith('Unnamed')]
        st.subheader("Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu cho Apriori")
        preprocess_method_apriori = st.radio(
            "Ch·ªçn ph∆∞∆°ng ph√°p ti·ªÅn x·ª≠ l√Ω:",
            ("One-hot encode c√°c c·ªôt ƒë∆∞·ª£c ch·ªçn", 
             "S·ª≠ d·ª•ng c√°c c·ªôt s·ªë g·ªëc", 
             "Chuy·ªÉn ƒë·ªïi to√†n b·ªô d·ªØ li·ªáu sang d·∫°ng boolean"),
            index=0, 
            key="apriori_preprocess_method"
        )

        df_apriori_processed = None
        if preprocess_method_apriori == "One-hot encode c√°c c·ªôt ƒë∆∞·ª£c ch·ªçn":
            st.info("Ph∆∞∆°ng ph√°p n√†y s·∫Ω one-hot encode c√°c c·ªôt b·∫°n ch·ªçn v√† s·ª≠ d·ª•ng ch√∫ng cho Apriori.")
            apriori_cols_to_encode = st.multiselect(
                "Ch·ªçn c√°c c·ªôt (categorical) ƒë·ªÉ one-hot encode:",
                options=df_apriori_input.columns.tolist(),
                key="apriori_cols_ohe"
            )
            if apriori_cols_to_encode:
                 df_apriori_processed = func.apriori_one_hot_encode_data(df_apriori_input, apriori_cols_to_encode)
            else:
                st.info("Vui l√≤ng ch·ªçn √≠t nh·∫•t m·ªôt c·ªôt ƒë·ªÉ one-hot encode, ho·∫∑c ch·ªçn ph∆∞∆°ng ph√°p kh√°c.")
        elif preprocess_method_apriori == "S·ª≠ d·ª•ng c√°c c·ªôt s·ªë g·ªëc (c·∫£nh b√°o!)":
            st.warning("ƒêang s·ª≠ d·ª•ng `process_data` g·ªëc cho Apriori: ch·ªâ ch·ªçn c√°c c·ªôt s·ªë. ƒêi·ªÅu n√†y c√≥ th·ªÉ kh√¥ng ph√π h·ª£p tr·ª´ khi c√°c c·ªôt s·ªë c·ªßa b·∫°n ƒë√£ l√† d·∫°ng 0/1 (giao d·ªãch).")
            df_apriori_processed = func.apriori_original_process_data(df_apriori_input)
        else:
            st.info("ƒêang chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu sang d·∫°ng boolean cho Apriori (True n·∫øu gi√° tr·ªã t·ªìn t·∫°i v√† kh√°c 0/False/chu·ªói r·ªóng).")
            df_apriori_processed = func.apriori_general_transactional_conversion(df_apriori_input)

        if df_apriori_processed is not None:
            st.write("D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω cho Apriori (5 d√≤ng ƒë·∫ßu):")
            st.dataframe(df_apriori_processed.head())

            st.subheader("Thi·∫øt l·∫≠p tham s·ªë Apriori")
            min_sup_apriori = st.slider("Ng∆∞·ª°ng H·ªó Tr·ª£ (Min Support)", 0.01, 1.0, 0.5, 0.01, key="apriori_minsup_slider")
            min_conf_apriori = st.slider("Ng∆∞·ª°ng Tin C·∫≠y (Min Confidence)", 0.01, 1.0, 0.3, 0.01, key="apriori_minconf_slider")

            if st.button("üöÄ Ch·∫°y Ph√¢n T√≠ch Apriori", key="apriori_run_button"):
                st.subheader("K·∫øt qu·∫£ Ph√¢n T√≠ch Apriori")
                try:
                    with st.spinner("ƒêang ch·∫°y Apriori..."):
                        f_itemsets, rules, max_f_itemsets = func.run_apriori_calculations(df_apriori_processed, min_sup_apriori, min_conf_apriori)
                    
                    st.markdown("##### T·∫≠p ph·ªï bi·∫øn:")
                    if not f_itemsets.empty:
                        f_itemsets_display = f_itemsets.copy()
                        f_itemsets_display['itemsets'] = f_itemsets_display['itemsets'].apply(lambda x: tuple(x))
                        st.dataframe(f_itemsets_display[['support', 'itemsets']].reset_index(drop=True))
                    else:
                        st.info("Kh√¥ng t√¨m th·∫•y t·∫≠p ph·ªï bi·∫øn n√†o.")

                    st.markdown("##### T·∫≠p ph·ªï bi·∫øn t·ªëi ƒë·∫°i:")
                    if not max_f_itemsets.empty:
                        max_f_itemsets_display = max_f_itemsets.copy()
                        max_f_itemsets_display['itemsets'] = max_f_itemsets_display['itemsets'].apply(lambda x: tuple(x))
                        st.dataframe(max_f_itemsets_display[['support', 'itemsets']].reset_index(drop=True))
                    else:
                        st.info("Kh√¥ng t√¨m th·∫•y t·∫≠p ph·ªï bi·∫øn t·ªëi ƒë·∫°i n√†o.")

                    st.markdown("##### C√°c lu·∫≠t k·∫øt h·ª£p:")
                    if not rules.empty:
                        rules_display = rules.copy()
                        rules_display['antecedents'] = rules_display['antecedents'].apply(lambda x: tuple(x))
                        rules_display['consequents'] = rules_display['consequents'].apply(lambda x: tuple(x))

                        st.dataframe(rules_display[['antecedents', 'consequents', 'support', 'confidence']].reset_index(drop=True))

                    else:
                        st.info("Kh√¥ng t√¨m th·∫•y lu·∫≠t k·∫øt h·ª£p n√†o.")

                except ValueError as ve: 
                    st.error(f"L·ªói Apriori: {ve}")
                except Exception as e:
                    st.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi ch·∫°y Apriori: {e}")
                
                st.markdown("---")

        else:
            st.info("D·ªØ li·ªáu ch∆∞a ƒë∆∞·ª£c x·ª≠ l√Ω cho Apriori. Vui l√≤ng ho√†n t·∫•t B∆∞·ªõc 1.")

    # ROUGH SET

    elif selected_analysis == analysis_options[2]:

        df_rs_input = df_original.copy()
        df_rs_input = df_rs_input.loc[:, ~df_rs_input.columns.str.startswith('Unnamed')]
        st.dataframe(df_rs_input)
        for col in df_rs_input.columns:
            if pd.api.types.is_numeric_dtype(df_rs_input[col]) and df_rs_input[col].nunique() > 15:
                st.sidebar.warning(f"C·ªôt s·ªë '{col}' c√≥ nhi·ªÅu gi√° tr·ªã duy nh·∫•t. C√¢n nh·∫Øc r·ªùi r·∫°c h√≥a cho Rough Set.")
            df_rs_input[col] = df_rs_input[col].astype(str) 

        st.subheader("Ch·ªçn Thu·ªôc T√≠nh cho Rough Set")
        all_cols_rs = df_rs_input.columns.tolist()
        decision_attribute_rs_name = st.selectbox("Ch·ªçn Thu·ªôc T√≠nh Quy·∫øt ƒê·ªãnh:", options=all_cols_rs, index=len(all_cols_rs)-1 if all_cols_rs else 0, key="rs_decision_attr_select")
        available_condition_attributes_rs_names = [col for col in all_cols_rs if col != decision_attribute_rs_name]
        
        if not available_condition_attributes_rs_names:
            st.warning("Kh√¥ng c√≤n thu·ªôc t√≠nh ƒëi·ªÅu ki·ªán.")
        else:
            selected_condition_attributes_rs_names = st.multiselect(
                "Ch·ªçn c√°c Thu·ªôc T√≠nh ƒêi·ªÅu Ki·ªán (cho x·∫•p x·ªâ/ph·ª• thu·ªôc ban ƒë·∫ßu):",
                options=available_condition_attributes_rs_names,
                default=available_condition_attributes_rs_names[:min(2, len(available_condition_attributes_rs_names))],
                key="rs_condition_attrs_select"
            )
            st.subheader("Ch·ªçn L·ªõp M·ª•c Ti√™u")
            if decision_attribute_rs_name and df_rs_input[decision_attribute_rs_name].nunique() > 0:
                target_class_values_rs_options = df_rs_input[decision_attribute_rs_name].unique().tolist()
                selected_target_class_value_rs = st.selectbox(f"Ch·ªçn L·ªõp M·ª•c Ti√™u (c·ªßa '{decision_attribute_rs_name}'):", options=target_class_values_rs_options, key="rs_target_class_select")

                if st.button(" Ch·∫°y Ph√¢n T√≠ch Rough Set", key="rs_run_button"):
                    st.subheader("K·∫øt qu·∫£ Ph√¢n T√≠ch Rough Set")
                    if not selected_condition_attributes_rs_names:
                        st.warning("Vui l√≤ng ch·ªçn √≠t nh·∫•t m·ªôt thu·ªôc t√≠nh ƒëi·ªÅu ki·ªán.")
                    else:
                        with st.spinner("ƒêang t√≠nh to√°n Rough Set..."):
                            lower_approx_rs = func.rs_lower_approximation(df_rs_input, selected_target_class_value_rs, selected_condition_attributes_rs_names, decision_attribute_rs_name)
                            upper_approx_rs = func.rs_upper_approximation(df_rs_input, selected_target_class_value_rs, selected_condition_attributes_rs_names, decision_attribute_rs_name)
                            accuracy_rs_val = func.rs_accuracy(df_rs_input, selected_target_class_value_rs, selected_condition_attributes_rs_names, decision_attribute_rs_name)
                            dependency_rs_val = func.rs_dependency(df_rs_input, selected_condition_attributes_rs_names, decision_attribute_rs_name)
                            all_possible_cond_attrs_for_reduct = available_condition_attributes_rs_names
                            reducts_rs_list = func.rs_find_all_reducts(df_rs_input, all_possible_cond_attrs_for_reduct, decision_attribute_rs_name)

                        st.write(f"**L·ªõp m·ª•c ti√™u:** `{selected_target_class_value_rs}` (t·ª´ `{decision_attribute_rs_name}`)")
                        st.write(f"**C√°c thu·ªôc t√≠nh ƒëi·ªÅu ki·ªán ƒëang x√©t:** `{', '.join(selected_condition_attributes_rs_names)}`")
                        st.write(f"**X·∫•p x·ªâ d∆∞·ªõi:** `{len(lower_approx_rs)}` ƒë·ªëi t∆∞·ª£ng. (Ch·ªâ s·ªë: `{sorted(list(lower_approx_rs)) if lower_approx_rs else 'R·ªóng'}`)")
                        st.write(f"**X·∫•p x·ªâ tr√™n:** `{len(upper_approx_rs)}` ƒë·ªëi t∆∞·ª£ng. (Ch·ªâ s·ªë: `{sorted(list(upper_approx_rs)) if upper_approx_rs else 'R·ªóng'}`)")
                        st.write(f"**ƒê·ªô ch√≠nh x√°c:** `{accuracy_rs_val:.4f}`")
                        st.write(f"**M·ª©c ƒë·ªô ph·ª• thu·ªôc:** `{dependency_rs_val:.4f}`")
                        st.subheader("C√°c R√∫t G·ªçn T·ªëi Thi·ªÉu")
                        if reducts_rs_list:
                            for r_idx, r_item in enumerate(reducts_rs_list): st.write(f"- R√∫t g·ªçn {r_idx+1}: `{r_item}`")
                        else: st.info("Kh√¥ng t√¨m th·∫•y r√∫t g·ªçn n√†o.")
                        st.markdown("---")

            else:
                st.info(f"Thu·ªôc t√≠nh quy·∫øt ƒë·ªãnh '{decision_attribute_rs_name}' c·∫ßn gi√° tr·ªã ƒë·ªÉ ph√¢n t√≠ch.")

    # DECISION TREE (ID3)
    elif selected_analysis == analysis_options[3]:

        df_dt_input = df_original.copy()
        df_dt_input = df_dt_input.loc[:, ~df_dt_input.columns.str.startswith('Unnamed')]
        st.subheader("Ch·ªçn Thu·ªôc T√≠nh cho C√¢y Quy·∫øt ƒê·ªãnh")
        all_cols_dt = df_dt_input.columns.tolist()
        
        target_attribute_dt_global = st.selectbox("Ch·ªçn Thu·ªôc T√≠nh M·ª•c Ti√™u (L·ªõp):", options=all_cols_dt, index=len(all_cols_dt)-1 if all_cols_dt else 0, key="dt_target_attr_select")
        available_features_dt_names = [col for col in all_cols_dt if col != target_attribute_dt_global]
        
        if not available_features_dt_names:
            st.warning("Kh√¥ng c√≤n thu·ªôc t√≠nh ƒë·∫ßu v√†o.")
        else:
            selected_feature_attributes_dt_names = st.multiselect("Ch·ªçn c√°c Thu·ªôc T√≠nh ƒê·∫ßu V√†o:", options=available_features_dt_names, default=available_features_dt_names, key="dt_feature_attrs_select")
            st.subheader("Ch·ªçn Ph∆∞∆°ng Ph√°p Chia Nh√°nh")
            method_dt_choice = st.radio("Ch·ªçn ph∆∞∆°ng ph√°p:", ('Gain (Entropy)', 'Gini Gain'), key="dt_method_radio")
            method_param_dt = 'Gain' if 'Gain (Entropy)' in method_dt_choice else 'Gini'

            if st.button(" X√¢y D·ª±ng C√¢y Quy·∫øt ƒê·ªãnh", key="dt_run_button"):
                st.subheader("K·∫øt qu·∫£ C√¢y Quy·∫øt ƒê·ªãnh")
                if not selected_feature_attributes_dt_names:
                    st.warning("Vui l√≤ng ch·ªçn √≠t nh·∫•t m·ªôt thu·ªôc t√≠nh ƒë·∫ßu v√†o.")
                elif target_attribute_dt_global:
                    with st.spinner("ƒêang x√¢y d·ª±ng c√¢y..."):
                        try:
                            df_subset_for_dt = df_dt_input[selected_feature_attributes_dt_names + [target_attribute_dt_global]].copy()
                            st.write("Chuy·ªÉn ƒë·ªïi c√°c c·ªôt ƒë√£ ch·ªçn sang d·∫°ng chu·ªói cho ID3:")
                            for col in df_subset_for_dt.columns:
                                df_subset_for_dt[col] = df_subset_for_dt[col].astype(str)
                                df_subset_for_dt = df_subset_for_dt[~df_subset_for_dt['Outlook'].isin(['nan'])]
                                st.caption(f"- C·ªôt '{col}' ƒë√£ sang chu·ªói.")
                            
                            st.markdown("##### ƒêi·ªÉm thu·ªôc t√≠nh (b∆∞·ªõc ƒë·∫ßu):")
                            for attr in selected_feature_attributes_dt_names:
                                score = func.dt_info_gain(df_subset_for_dt, attr, target_attribute_dt_global) if method_param_dt == 'Gain' else func.dt_gini_gain(df_subset_for_dt, attr, target_attribute_dt_global)
                                st.write(f"- **{attr}**: {method_param_dt} = `{score:.4f}`")
                            
                            decision_tree_model = func.dt_build_tree(df_subset_for_dt, selected_feature_attributes_dt_names, target_attribute_dt_global, method_param_dt)
                            st.success("‚úÖ C√¢y ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng!")
                            st.markdown("##### H√¨nh ·∫£nh C√¢y:")
                            try:
                                graphviz_dot_obj = func.dt_draw_tree_graphviz(decision_tree_model, target_attribute_dt_global) 
                                st.graphviz_chart(graphviz_dot_obj)
                            except Exception as e_graph: st.error(f"L·ªói v·∫Ω c√¢y: {e_graph}.")
                            st.markdown("##### C√°c Lu·∫≠t R√∫t Ra:")
                            extracted_rules_dt_list = func.dt_extract_rules(decision_tree_model, target_attribute_dt_global) 
                            if extracted_rules_dt_list:
                                for rule_item_str in extracted_rules_dt_list: st.markdown(f"- {rule_item_str}")
                            else: st.info("Kh√¥ng c√≥ lu·∫≠t n√†o.")
                        except Exception as e_dt_build: st.error(f"L·ªói x√¢y d·ª±ng c√¢y: {e_dt_build}")
                    st.markdown("---")
                else:
                    st.warning("Vui l√≤ng ch·ªçn thu·ªôc t√≠nh m·ª•c ti√™u.")
    
    # NAIVE BAYES, K-MEANS, KOHONEN SOM (T·ª´ script th·ª© 4)
    df_nbkm_input = df_original.copy()
    df_nbkm_input = df_nbkm_input.loc[:, ~df_nbkm_input.columns.str.startswith('Unnamed')]
    X_processed_nbkm, y_processed_for_nb_only, le_y_for_nb_only, feature_encoders_nbkm = None, None, None, None
    
    st.sidebar.markdown("---")
    st.sidebar.subheader("T√πy ch·ªçn cho Naive Bayes / K-Means / SOM")
    target_column_for_nb_som = st.sidebar.selectbox("Ch·ªçn C·ªôt M·ª•c Ti√™u (cho Naive Bayes, t√πy ch·ªçn cho SOM):", options=[None] + df_nbkm_input.columns.tolist(), index=0, key="nbkm_target_col_select")

    try:
        X_processed_nbkm, y_processed_for_nb_only, le_y_for_nb_only, feature_encoders_nbkm = \
            func.general_preprocess_data_nbkm(df_nbkm_input, target_column_for_nb_som, st_instance=st)

        if X_processed_nbkm is None and selected_analysis != analysis_options[0] and selected_analysis != analysis_options[1] and selected_analysis != analysis_options[2] and selected_analysis != analysis_options[3]:
            st.error("L·ªói ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu chung. Kh√¥ng th·ªÉ ti·∫øp t·ª•c.")
        else:
            if selected_analysis != analysis_options[0] and selected_analysis != analysis_options[1] and selected_analysis != analysis_options[2] and selected_analysis != analysis_options[3]: 
                st.markdown("#### D·ªØ li·ªáu thu·ªôc t√≠nh ƒë√£ x·ª≠ l√Ω (X - 5 d√≤ng ƒë·∫ßu):")
                st.dataframe(X_processed_nbkm.head())
                if y_processed_for_nb_only is not None and target_column_for_nb_som:
                    st.markdown(f"#### Nh√£n m·ª•c ti√™u '{target_column_for_nb_som}' ƒë√£ x·ª≠ l√Ω (y - 5 gi√° tr·ªã ƒë·∫ßu):")
                    st.write(y_processed_for_nb_only[:5])
                    if le_y_for_nb_only: st.caption(f"C√°c l·ªõp g·ªëc: {list(le_y_for_nb_only.classes_)}")
            
            # --- NAIVE BAYES ---
            if selected_analysis == analysis_options[4]:

                if y_processed_for_nb_only is None or target_column_for_nb_som is None:
                    st.warning("‚ö†Ô∏è Vui l√≤ng ch·ªçn c·ªôt m·ª•c ti√™u ·ªü thanh b√™n cho Naive Bayes.")
                else:
                    st.subheader("Ch·ªçn lo·∫°i Naive Bayes")
                    nb_method_type_choice = st.radio("Ch·ªçn ph∆∞∆°ng ph√°p:", ("GaussianNB", "MultinomialNB"), key="nb_method_radio_choice")
                    method_map = {"GaussianNB": "gaussian", "MultinomialNB": "multinomial"}
                    nb_method_internal = method_map[nb_method_type_choice]

                    alpha_nb = 1.0
                    X_nb_final = X_processed_nbkm.copy() 
                    can_train_multinomial = True

                    if nb_method_type_choice == "MultinomialNB":
                        alpha_nb = st.slider("Alpha (Laplace smoothing):", 0.01, 3.0, 0.5, 0.1, key="nb_alpha_slider")
                        if (X_nb_final < 0).any().any():
                            st.warning("MultinomialNB y√™u c·∫ßu ƒë·∫∑c tr∆∞ng kh√¥ng √¢m.")
                            if st.checkbox("√Åp d·ª•ng MinMaxScaler?", key="nb_minmax_multi_check"):
                                scaler_nb_multi = MinMaxScaler()
                                X_nb_final = scaler_nb_multi.fit_transform(X_nb_final)
                                st.info("ƒê√£ √°p d·ª•ng MinMaxScaler.")
                            else:
                                st.error("Kh√¥ng th·ªÉ hu·∫•n luy·ªán MultinomialNB v·ªõi gi√° tr·ªã √¢m.")
                                can_train_multinomial = False
                    
                    if st.button(" Hu·∫•n Luy·ªán Naive Bayes", key="nb_run_button"):
                        st.subheader("K·∫øt qu·∫£ Naive Bayes")
                        if nb_method_type_choice == "MultinomialNB" and not can_train_multinomial:
                            st.error("Hu·∫•n luy·ªán MultinomialNB b·ªã h·ªßy do c√≥ gi√° tr·ªã √¢m v√† kh√¥ng √°p d·ª•ng scaling.")
                        else:
                            with st.spinner(f"ƒêang hu·∫•n luy·ªán {nb_method_type_choice}..."):
                                try:
                                    model_nb_trained, acc_nb_trained = func.train_naive_bayes_model(
                                        pd.DataFrame(X_nb_final, columns=X_processed_nbkm.columns) if isinstance(X_nb_final, np.ndarray) else X_nb_final, 
                                        y_processed_for_nb_only, 
                                        nb_method_internal, 
                                        alpha_nb,
                                        st_instance=st 
                                    )
                                    if model_nb_trained:
                                        st.success(f"‚úÖ {nb_method_type_choice} ƒë√£ hu·∫•n luy·ªán.")
                                        st.metric(label=f"ƒê·ªô ch√≠nh x√°c (tr√™n t·∫≠p hu·∫•n luy·ªán)", value=f"{acc_nb_trained:.4f}")
                                    else: st.error(f"Kh√¥ng th·ªÉ hu·∫•n luy·ªán {nb_method_type_choice}.")
                                except ValueError as ve_nb_train: 
                                    st.error(f"L·ªói hu·∫•n luy·ªán Naive Bayes: {ve_nb_train}")
                                except Exception as e_nb_train:
                                    st.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi hu·∫•n luy·ªán: {e_nb_train}")
                        st.markdown("---")
            
            # --- K-MEANS CLUSTERING ---
            elif selected_analysis == analysis_options[5]:
                st.subheader("Ch·ªçn S·ªë C·ª•m (K)")
                num_k_clusters_choice = st.slider("Ch·ªçn s·ªë c·ª•m (K):", 2, 10, 3, 1, key="kmeans_k_slider")
                if st.button(" Ch·∫°y K-Means", key="kmeans_run_button"):
                    st.subheader("K·∫øt qu·∫£ K-Means")
                    with st.spinner(f"ƒêang ch·∫°y K-Means (K={num_k_clusters_choice})..."):
                        try:
                            labels_km_res, centers_km_res, df_clustered_km_res, X_scaled_for_kmeans_plot = \
                                func.run_kmeans_clustering_analysis(X_processed_nbkm, num_k_clusters_choice)
                            
                            if labels_km_res is not None:
                                st.write(f"K·∫øt qu·∫£ g√°n nh√£n c·ª•m:")
                                st.dataframe(df_clustered_km_res.head())
                                if X_processed_nbkm.shape[1] >= 2 and X_scaled_for_kmeans_plot is not None:
                                    fig_km, ax_km = plt.subplots(figsize=(8, 6))
                                    ax_km.scatter(X_scaled_for_kmeans_plot[:, 0], X_scaled_for_kmeans_plot[:, 1], c=labels_km_res, cmap='viridis', alpha=0.7, edgecolors='k')
                                    ax_km.scatter(centers_km_res[:, 0], centers_km_res[:, 1], marker='X', s=100, color='red', label='T√¢m c·ª•m')
                                    ax_km.set_xlabel(f"{X_processed_nbkm.columns[0]} (chu·∫©n h√≥a)")
                                    ax_km.set_ylabel(f"{X_processed_nbkm.columns[1]} (chu·∫©n h√≥a)")
                                    ax_km.set_title(f"K-Means (K={num_k_clusters_choice})")
                                    ax_km.legend()
                                    st.pyplot(fig_km)
                                else: st.info("C·∫ßn √≠t nh·∫•t 2 thu·ªôc t√≠nh ƒë·ªÉ v·∫Ω 2D.")
                            else: st.error("L·ªói K-Means.")
                        except ValueError as ve_km: st.error(f"L·ªói K-Means: {ve_km}")
                        except Exception as e_km: st.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh K-Means: {e_km}")
                    st.markdown("---")

            # --- KOHONEN SOM ---
            elif selected_analysis == analysis_options[6]:
                st.subheader("Thi·∫øt l·∫≠p Tham s·ªë SOM")
                som_grid_r = st.number_input("S·ªë h√†ng l∆∞·ªõi SOM:", 3, 25, 4, 1, key="som_r")
                som_grid_c = st.number_input("S·ªë c·ªôt l∆∞·ªõi SOM:", 3, 25, 4, 1, key="som_c")
                som_sig = st.slider("Sigma:", 0.1, 5.0, 1.0, 0.1, key="som_sig")
                som_lrn = st.slider("Learning Rate:", 0.01, 1.0, 0.5, 0.01, key="som_lrn")
                som_iter = st.number_input("S·ªë v√≤ng l·∫∑p:", 100, 10000, 500, 100, key="som_iter")
                num_som_rand_cls = st.number_input("S·ªë 'l·ªõp g√°n ng·∫´u nhi√™n' (script g·ªëc):", 1, 10, 3, 1, key="som_rand_cls")

                if st.button(" Hu·∫•n Luy·ªán SOM", key="som_run_button"):
                    st.subheader("K·∫øt qu·∫£ SOM")
                    with st.spinner("ƒêang hu·∫•n luy·ªán SOM..."):
                        try:
                            som_model, X_scaled_som = func.train_kohonen_som_model(
                                X_processed_nbkm, som_grid_r, som_grid_c, som_sig, som_lrn, som_iter, st_instance=st
                            )
                            if som_model and X_scaled_som is not None:
                                st.markdown("### Hit Map (SOM)")
                                act_resp_som = som_model.activation_response(X_scaled_som).T
                                fig_som_h, ax_som_h = plt.subplots(figsize=(som_grid_c*0.5, som_grid_r*0.5))
                                im_h = ax_som_h.pcolor(act_resp_som, cmap='viridis')
                                ax_som_h.set_title('SOM - Hit Map')
                                fig_som_h.colorbar(im_h, ax=ax_som_h)
                                st.pyplot(fig_som_h)

                                st.markdown(f"### B·∫£n ƒë·ªì theo {num_som_rand_cls} 'L·ªõp G√°n'")
                                som_rand_lbls = np.random.randint(0, num_som_rand_cls, size=X_scaled_som.shape[0])
                                n_c_fig_sr = min(num_som_rand_cls, 3)
                                n_r_fig_sr = math.ceil(num_som_rand_cls / n_c_fig_sr)
                                fig_sr, axs_sr = plt.subplots(n_r_fig_sr, n_c_fig_sr, figsize=(n_c_fig_sr*4, n_r_fig_sr*3.5))
                                if num_som_rand_cls == 1: axs_sr = [axs_sr]
                                else: axs_sr = axs_sr.flatten()
                                uniq_rand_lbls = np.unique(som_rand_lbls)
                                for i_p, ax_p_sr in enumerate(axs_sr):
                                    if i_p < num_som_rand_cls:
                                        cur_r_lbl = uniq_rand_lbls[i_p] if i_p < len(uniq_rand_lbls) else -1
                                        if cur_r_lbl != -1:
                                            data_r_lbl = X_scaled_som[som_rand_lbls == cur_r_lbl]
                                            if data_r_lbl.shape[0]>0:
                                                win_map_r = som_model.win_map(data_r_lbl)
                                                heatmap_r = np.zeros((som_grid_r, som_grid_c))
                                                for r_i_h, c_i_h in win_map_r.keys(): heatmap_r[r_i_h,c_i_h] = len(win_map_r[(r_i_h,c_i_h)])
                                                sns.heatmap(heatmap_r.T, ax=ax_p_sr, cmap="coolwarm", annot=True, fmt=".0f", cbar=i_p==0)
                                                ax_p_sr.set_title(f"L·ªõp: {cur_r_lbl}")
                                            else: ax_p_sr.set_title(f"L·ªõp: {cur_r_lbl} (R·ªóng)")
                                        else: ax_p_sr.axis("off")
                                    else: ax_p_sr.axis("off")
                                plt.tight_layout()
                                st.pyplot(fig_sr)
                            else: st.error("L·ªói hu·∫•n luy·ªán SOM.")
                        except ValueError as ve_som: st.error(f"L·ªói SOM: {ve_som}")
                        except Exception as e_som: st.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh SOM: ƒêi·ªÅu ch·ªânh h√†ng v√† c·ªôt ph√π h·ª£p.")
                    st.markdown("---")
            
            elif selected_analysis in [analysis_options[4], analysis_options[5], analysis_options[6]] and X_processed_nbkm is None:
                 st.error("L·ªói ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu chung t·∫°i NB, KM, SOM.")

    except Exception as e_general_preprocess: 
        st.error(f"L·ªói trong qu√° tr√¨nh ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu chung: {e_general_preprocess}")


elif uploaded_file is None and selected_analysis != analysis_options[0]:
    st.info(" Vui l√≤ng t·∫£i l√™n m·ªôt t·ªáp CSV t·ª´ thanh b√™n ƒë·ªÉ b·∫Øt ƒë·∫ßu ph√¢n t√≠ch.")
elif selected_analysis == analysis_options[0] and uploaded_file is not None:
    st.info("D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c t·∫£i. Vui l√≤ng ch·ªçn m·ªôt thu·∫≠t to√°n ph√¢n t√≠ch t·ª´ thanh b√™n.")
elif selected_analysis == analysis_options[0] and uploaded_file is None:
    st.info("H√£y t·∫£i t·ªáp CSV v√† ch·ªçn m·ªôt thu·∫≠t to√°n ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

st.sidebar.markdown("---")

