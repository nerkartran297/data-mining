# ğŸ“š HÆ¯á»šNG DáºªN CÃC THUáº¬T TOÃN KHAI THÃC Dá»® LIá»†U

## ğŸ“‹ Má»¥c lá»¥c

1. [Apriori - Luáº­t Káº¿t Há»£p](#1-apriori---luáº­t-káº¿t-há»£p)
2. [Rough Set - LÃ½ Thuyáº¿t Táº­p ThÃ´](#2-rough-set---lÃ½-thuyáº¿t-táº­p-thÃ´)
3. [Decision Tree ID3 - CÃ¢y Quyáº¿t Äá»‹nh](#3-decision-tree-id3---cÃ¢y-quyáº¿t-Ä‘á»‹nh)
4. [Naive Bayes - PhÃ¢n Loáº¡i XÃ¡c Suáº¥t](#4-naive-bayes---phÃ¢n-loáº¡i-xÃ¡c-suáº­t)
5. [K-Means - PhÃ¢n Cá»¥m](#5-k-means---phÃ¢n-cá»¥m)
6. [Kohonen SOM - Báº£n Äá»“ Tá»± Tá»• Chá»©c](#6-kohonen-som---báº£n-Ä‘á»“-tá»±-tá»•-chá»©c)

---

# 1. APRIORI - LUáº¬T Káº¾T Há»¢P

## ğŸŒŸ PhiÃªn báº£n Dá»„ HIá»‚U

### Apriori lÃ  gÃ¬?

HÃ£y tÆ°á»Ÿng tÆ°á»£ng báº¡n lÃ  chá»§ siÃªu thá»‹ vÃ  muá»‘n biáº¿t khÃ¡ch hÃ ng thÆ°á»ng mua nhá»¯ng gÃ¬ cÃ¹ng nhau!

**VÃ­ dá»¥ Ä‘á»i thÆ°á»ng:**

```
ğŸ›’ Giá» hÃ ng 1: BÃ¡nh mÃ¬ + Sá»¯a + BÆ¡
ğŸ›’ Giá» hÃ ng 2: BÃ¡nh mÃ¬ + Sá»¯a
ğŸ›’ Giá» hÃ ng 3: Sá»¯a + PhÃ´ mai
ğŸ›’ Giá» hÃ ng 4: BÃ¡nh mÃ¬ + Sá»¯a + PhÃ´ mai
```

**Apriori sáº½ tÃ¬m ra:**

- "Náº¿u ai mua **BÃ¡nh mÃ¬** thÃ¬ **80%** cÅ©ng mua **Sá»¯a**"
- "Náº¿u ai mua **Sá»¯a** vÃ  **BÃ¡nh mÃ¬** thÃ¬ **60%** cÅ©ng mua **BÆ¡**"

### Táº¡i sao quan trá»ng?

- ğŸ¯ **Bá»‘ trÃ­ hÃ ng hÃ³a**: Äáº·t sá»¯a gáº§n bÃ¡nh mÃ¬
- ğŸ“ˆ **Khuyáº¿n mÃ£i**: Giáº£m giÃ¡ combo bÃ¡nh mÃ¬ + sá»¯a
- ğŸ’¡ **Gá»£i Ã½ mua hÃ ng**: "KhÃ¡ch hÃ ng Ä‘Ã£ mua bÃ¡nh mÃ¬, cÃ³ thá»ƒ thÃ­ch sá»¯a"

### CÃ¡c khÃ¡i niá»‡m Ä‘Æ¡n giáº£n:

- **Support (Há»— trá»£)**: CÃ³ bao nhiá»u % khÃ¡ch hÃ ng mua item nÃ y?
  - VD: 70% khÃ¡ch mua bÃ¡nh mÃ¬ â†’ Support(BÃ¡nh mÃ¬) = 0.7
- **Confidence (Tin cáº­y)**: Náº¿u mua A thÃ¬ kháº£ nÄƒng mua B lÃ  bao nhiá»u?
  - VD: Trong sá»‘ ngÆ°á»i mua bÃ¡nh mÃ¬, 80% cÅ©ng mua sá»¯a â†’ Conf(BÃ¡nh mÃ¬ â†’ Sá»¯a) = 0.8

## ğŸ”¬ PhiÃªn báº£n TECHNICAL

### Äá»‹nh nghÄ©a chÃ­nh thá»©c:

Apriori lÃ  thuáº­t toÃ¡n tÃ¬m **frequent itemsets** vÃ  **association rules** tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u giao dá»‹ch.

### CÃ¡c bÆ°á»›c thuáº­t toÃ¡n:

1. **TÃ¬m frequent 1-itemsets** (Lâ‚)
2. **Sinh candidate 2-itemsets** (Câ‚‚) tá»« Lâ‚
3. **TÃ­nh support vÃ  lá»c** â†’ Lâ‚‚
4. **Láº·p láº¡i** cho Ä‘áº¿n khi khÃ´ng cÃ²n frequent itemsets

### CÃ´ng thá»©c toÃ¡n há»c:

**Support:**

```
Support(A) = |T(A)| / |T|
```

Trong Ä‘Ã³: |T(A)| = sá»‘ giao dá»‹ch chá»©a A, |T| = tá»•ng sá»‘ giao dá»‹ch

**Confidence:**

```
Confidence(A â†’ B) = Support(A âˆª B) / Support(A)
```

**Lift:**

```
Lift(A â†’ B) = Support(A âˆª B) / (Support(A) Ã— Support(B))
```

### Pseudocode:

```python
def apriori(transactions, min_support):
    L1 = find_frequent_1_itemsets(transactions, min_support)
    L = [L1]
    k = 2

    while L[k-2] is not empty:
        Ck = apriori_gen(L[k-2])  # Generate candidates
        for transaction in transactions:
            Ct = subset(Ck, transaction)  # Candidates in transaction
            for candidate in Ct:
                candidate.count++

        Lk = {c in Ck | c.support >= min_support}
        L.append(Lk)
        k = k + 1

    return union(L)
```

---

# 2. ROUGH SET - LÃ THUYáº¾T Táº¬P THÃ”

## ğŸŒŸ PhiÃªn báº£n Dá»„ HIá»‚U

### Rough Set lÃ  gÃ¬?

HÃ£y tÆ°á»Ÿng tÆ°á»£ng báº¡n muá»‘n phÃ¢n loáº¡i há»c sinh "Giá»i" hay "Yáº¿u" dá»±a trÃªn Ä‘iá»ƒm sá»‘, nhÆ°ng cÃ³ má»™t sá»‘ trÆ°á»ng há»£p khÃ´ng rÃµ rÃ ng!

**VÃ­ dá»¥:**

```
Há»c sinh A: ToÃ¡n=8, LÃ½=7, HÃ³a=9 â†’ Giá»i âœ“
Há»c sinh B: ToÃ¡n=5, LÃ½=4, HÃ³a=6 â†’ Yáº¿u âœ“
Há»c sinh C: ToÃ¡n=7, LÃ½=6, HÃ³a=5 â†’ ??? (KhÃ´ng cháº¯c)
```

### Rough Set giÃºp gÃ¬?

- ğŸ¯ **TÃ¬m thuá»™c tÃ­nh quan trá»ng**: MÃ´n nÃ o quyáº¿t Ä‘á»‹nh "Giá»i/Yáº¿u"?
- ğŸ” **Xá»­ lÃ½ khÃ´ng cháº¯c cháº¯n**: PhÃ¢n loáº¡i nhá»¯ng trÆ°á»ng há»£p mÆ¡ há»“
- âœ‚ï¸ **Giáº£m thuá»™c tÃ­nh**: Loáº¡i bá» thÃ´ng tin thá»«a

### CÃ¡c khÃ¡i niá»‡m Ä‘Æ¡n giáº£n:

- **Lower Approximation (Xáº¥p xá»‰ dÆ°á»›i)**: Nhá»¯ng trÆ°á»ng há»£p **CHáº®C CHáº®N** thuá»™c nhÃ³m
- **Upper Approximation (Xáº¥p xá»‰ trÃªn)**: Nhá»¯ng trÆ°á»ng há»£p **CÃ“ THá»‚** thuá»™c nhÃ³m
- **Boundary Region (VÃ¹ng biÃªn)**: Nhá»¯ng trÆ°á»ng há»£p **KHÃ”NG CHáº®C**

### Minh há»a trá»±c quan:

```
ğŸ¯ Má»¥c tiÃªu: PhÃ¢n loáº¡i "Há»c sinh giá»i"

Lower Approximation (Cháº¯c cháº¯n giá»i):
ğŸ‘¨â€ğŸ“ ToÃ¡nâ‰¥8 AND LÃ½â‰¥8 â†’ 100% Giá»i

Upper Approximation (CÃ³ thá»ƒ giá»i):
ğŸ‘¨â€ğŸ“ ToÃ¡nâ‰¥6 OR LÃ½â‰¥6 â†’ CÃ³ thá»ƒ Giá»i

Boundary Region (KhÃ´ng cháº¯c):
ğŸ‘¨â€ğŸ“ 6â‰¤ToÃ¡n<8 AND 6â‰¤LÃ½<8 â†’ Cáº§n xem thÃªm
```

## ğŸ”¬ PhiÃªn báº£n TECHNICAL

### Äá»‹nh nghÄ©a toÃ¡n há»c:

Cho táº­p vÅ© trá»¥ U, quan há»‡ tÆ°Æ¡ng Ä‘Æ°Æ¡ng R, vÃ  táº­p má»¥c tiÃªu X âŠ† U.

**Lower Approximation:**

```
R*(X) = {x âˆˆ U | [x]R âŠ† X}
```

**Upper Approximation:**

```
R*(X) = {x âˆˆ U | [x]R âˆ© X â‰  âˆ…}
```

**Boundary Region:**

```
BND_R(X) = R*(X) - R*(X)
```

### CÃ¡c thÆ°á»›c Ä‘o cháº¥t lÆ°á»£ng:

**Accuracy (Äá»™ chÃ­nh xÃ¡c):**

```
Î±_R(X) = |R*(X)| / |R*(X)|
```

**Dependency (Má»©c Ä‘á»™ phá»¥ thuá»™c):**

```
Î³_R(D) = |POS_R(D)| / |U|
```

### Reduct vÃ  Core:

- **Reduct**: Táº­p con tá»‘i thiá»ƒu cá»§a thuá»™c tÃ­nh váº«n giá»¯ nguyÃªn kháº£ nÄƒng phÃ¢n loáº¡i
- **Core**: Giao cá»§a táº¥t cáº£ cÃ¡c reduct (thuá»™c tÃ­nh khÃ´ng thá»ƒ thiáº¿u)

---

# 3. DECISION TREE ID3 - CÃ‚Y QUYáº¾T Äá»ŠNH

## ğŸŒŸ PhiÃªn báº£n Dá»„ HIá»‚U

### Decision Tree lÃ  gÃ¬?

NhÆ° má»™t cÃ¢y cÃ¢u há»i Ä‘á»ƒ Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh! Má»—i nÃºt lÃ  má»™t cÃ¢u há»i, má»—i nhÃ¡nh lÃ  má»™t Ä‘Ã¡p Ã¡n.

**VÃ­ dá»¥: CÃ³ nÃªn Ä‘i chÆ¡i khÃ´ng?**

```
ğŸŒ¤ï¸ Thá»i tiáº¿t nhÆ° tháº¿ nÃ o?
â”œâ”€â”€ â˜€ï¸ Náº¯ng
â”‚   â””â”€â”€ ğŸ˜Š Äi chÆ¡i!
â”œâ”€â”€ ğŸŒ§ï¸ MÆ°a
â”‚   â””â”€â”€ ğŸ  á» nhÃ 
â””â”€â”€ â˜ï¸ Nhiá»u mÃ¢y
    â””â”€â”€ ğŸ’° CÃ³ tiá»n khÃ´ng?
        â”œâ”€â”€ ğŸ’¸ CÃ³ â†’ ğŸ˜Š Äi chÆ¡i!
        â””â”€â”€ ğŸ’¸ KhÃ´ng â†’ ğŸ  á» nhÃ 
```

### Táº¡i sao dÃ¹ng Decision Tree?

- ğŸ“– **Dá»… hiá»ƒu**: NhÆ° sÃ¡ch hÆ°á»›ng dáº«n tá»«ng bÆ°á»›c
- ğŸš€ **Nhanh**: Quyáº¿t Ä‘á»‹nh trong vÃ i giÃ¢y
- ğŸ¯ **ChÃ­nh xÃ¡c**: Dá»±a trÃªn dá»¯ liá»‡u thá»±c táº¿

### CÃ¡ch hoáº¡t Ä‘á»™ng:

1. **Chá»n cÃ¢u há»i tá»‘t nháº¥t** (thuá»™c tÃ­nh quan trá»ng nháº¥t)
2. **Chia dá»¯ liá»‡u** theo cÃ¢u tráº£ lá»i
3. **Láº·p láº¡i** cho Ä‘áº¿n khi cÃ³ káº¿t quáº£ rÃµ rÃ ng

## ğŸ”¬ PhiÃªn báº£n TECHNICAL

### Thuáº­t toÃ¡n ID3:

ID3 sá»­ dá»¥ng **Information Gain** Ä‘á»ƒ chá»n thuá»™c tÃ­nh tá»‘t nháº¥t cho má»—i nÃºt.

**Entropy:**

```
Entropy(S) = -âˆ‘(p_i Ã— logâ‚‚(p_i))
```

**Information Gain:**

```
Gain(S,A) = Entropy(S) - âˆ‘((|S_v|/|S|) Ã— Entropy(S_v))
```

### Pseudocode:

```python
def ID3(examples, target_attribute, attributes):
    # Base cases
    if all examples have same target value:
        return leaf node with that value

    if attributes is empty:
        return leaf node with majority target value

    # Choose best attribute
    best_attr = argmax(information_gain(examples, attr, target))

    # Create decision node
    tree = DecisionNode(best_attr)

    # Recursively build subtrees
    for value in values(best_attr):
        subset = examples where best_attr = value
        if subset is empty:
            add leaf with majority value
        else:
            remaining_attrs = attributes - {best_attr}
            subtree = ID3(subset, target_attribute, remaining_attrs)
            tree.add_branch(value, subtree)

    return tree
```

### Æ¯u vÃ  nhÆ°á»£c Ä‘iá»ƒm:

**Æ¯u Ä‘iá»ƒm:**

- Dá»… hiá»ƒu vÃ  giáº£i thÃ­ch
- KhÃ´ng cáº§n giáº£ Ä‘á»‹nh vá» phÃ¢n phá»‘i dá»¯ liá»‡u
- Xá»­ lÃ½ Ä‘Æ°á»£c dá»¯ liá»‡u categorical

**NhÆ°á»£c Ä‘iá»ƒm:**

- Dá»… overfitting
- ThiÃªn vá»‹ vá»›i thuá»™c tÃ­nh cÃ³ nhiá»u giÃ¡ trá»‹
- KhÃ´ng xá»­ lÃ½ Ä‘Æ°á»£c dá»¯ liá»‡u thiáº¿u

---

# 4. NAIVE BAYES - PHÃ‚N LOáº I XÃC SUáº¤T

## ğŸŒŸ PhiÃªn báº£n Dá»„ HIá»‚U

### Naive Bayes lÃ  gÃ¬?

NhÆ° má»™t bÃ¡c sÄ© cháº©n Ä‘oÃ¡n bá»‡nh dá»±a trÃªn triá»‡u chá»©ng!

**VÃ­ dá»¥: PhÃ¢n loáº¡i email spam**

```
ğŸ“§ Email cÃ³ tá»« "FREE" â†’ 80% lÃ  spam
ğŸ“§ Email cÃ³ tá»« "MONEY" â†’ 70% lÃ  spam
ğŸ“§ Email cÃ³ tá»« "URGENT" â†’ 60% lÃ  spam

ğŸ“© Email má»›i: "FREE MONEY URGENT!!!"
â†’ XÃ¡c suáº¥t spam ráº¥t cao!
```

### Táº¡i sao gá»i lÃ  "Naive" (NgÃ¢y thÆ¡)?

VÃ¬ thuáº­t toÃ¡n **giáº£ Ä‘á»‹nh** cÃ¡c Ä‘áº·c trÆ°ng Ä‘á»™c láº­p vá»›i nhau (Ä‘iá»u nÃ y thÆ°á»ng khÃ´ng Ä‘Ãºng trong thá»±c táº¿).

**VÃ­ dá»¥ ngÃ¢y thÆ¡:**

- Giáº£ Ä‘á»‹nh: Tá»« "FREE" vÃ  "MONEY" xuáº¥t hiá»‡n Ä‘á»™c láº­p
- Thá»±c táº¿: Spam thÆ°á»ng dÃ¹ng cáº£ hai tá»« cÃ¹ng lÃºc

### CÃ¡c loáº¡i Naive Bayes:

- **Gaussian NB**: Cho dá»¯ liá»‡u sá»‘ (chiá»u cao, cÃ¢n náº·ng...)
- **Multinomial NB**: Cho dá»¯ liá»‡u Ä‘áº¿m (sá»‘ tá»« trong email...)

## ğŸ”¬ PhiÃªn báº£n TECHNICAL

### Äá»‹nh lÃ½ Bayes:

```
P(C|X) = P(X|C) Ã— P(C) / P(X)
```

Trong Ä‘Ã³:

- P(C|X): XÃ¡c suáº¥t thuá»™c class C khi biáº¿t features X
- P(X|C): Likelihood - xÃ¡c suáº¥t cÃ³ features X khi thuá»™c class C
- P(C): Prior - xÃ¡c suáº¥t ban Ä‘áº§u cá»§a class C
- P(X): Evidence - xÃ¡c suáº¥t cÃ³ features X

### Giáº£ Ä‘á»‹nh Ä‘á»™c láº­p:

```
P(xâ‚,xâ‚‚,...,xâ‚™|C) = P(xâ‚|C) Ã— P(xâ‚‚|C) Ã— ... Ã— P(xâ‚™|C)
```

### CÃ´ng thá»©c tá»•ng quÃ¡t:

```
Ä‰ = argmax P(c) âˆ P(xáµ¢|c)
    câˆˆC         i=1
```

### CÃ¡c biáº¿n thá»ƒ:

**Gaussian Naive Bayes:**

```
P(xáµ¢|c) = (1/âˆš(2Ï€ÏƒÂ²c)) Ã— exp(-(xáµ¢-Î¼c)Â²/(2ÏƒÂ²c))
```

**Multinomial Naive Bayes:**

```
P(xáµ¢|c) = (Náµ¢c + Î±) / (Nc + Î±|V|)
```

---

# 5. K-MEANS - PHÃ‚N Cá»¤M

## ğŸŒŸ PhiÃªn báº£n Dá»„ HIá»‚U

### K-Means lÃ  gÃ¬?

NhÆ° viá»‡c chia há»c sinh thÃ nh cÃ¡c nhÃ³m dá»±a trÃªn Ä‘iá»ƒm sá»‘!

**VÃ­ dá»¥: Chia khÃ¡ch hÃ ng thÃ nh 3 nhÃ³m**

```
ğŸ‘¥ NhÃ³m 1: KhÃ¡ch hÃ ng VIP (thu nháº­p cao, mua nhiá»u)
ğŸ‘¥ NhÃ³m 2: KhÃ¡ch hÃ ng thÆ°á»ng (thu nháº­p trung bÃ¬nh)
ğŸ‘¥ NhÃ³m 3: KhÃ¡ch hÃ ng má»›i (thu nháº­p tháº¥p, Ã­t mua)
```

### CÃ¡ch hoáº¡t Ä‘á»™ng (nhÆ° chÆ¡i game):

1. **ğŸ¯ Äáº·t K tÃ¢m cá»¥m ngáº«u nhiÃªn** (K = 3 tÃ¢m)
2. **ğŸ‘¥ GÃ¡n má»—i ngÆ°á»i vÃ o nhÃ³m gáº§n nháº¥t**
3. **ğŸ“ Di chuyá»ƒn tÃ¢m vá» giá»¯a nhÃ³m**
4. **ğŸ”„ Láº·p láº¡i** cho Ä‘áº¿n khi á»•n Ä‘á»‹nh

### Minh há»a trá»±c quan:

```
BÆ°á»›c 1: TÃ¢m ngáº«u nhiÃªn        BÆ°á»›c 2: GÃ¡n nhÃ³m
   *     â€¢  â€¢                   *â—â—   â—‹ â—‹
 â€¢   â€¢     â€¢                  â—   â—   â—‹ â—‹
   â€¢   *     â€¢ â€¢                â—   *â—‹   â—‹ â—‹
     â€¢   â€¢  *                    â—   â—‹â—‹  *â—‹

BÆ°á»›c 3: Di chuyá»ƒn tÃ¢m          Káº¿t quáº£ cuá»‘i:
   *â—â—   â—‹ â—‹                    *â—â—   â—‹ â—‹
 â—   â—   â—‹ â—‹                  â—   â—   â—‹ â—‹
   â—   *â—‹   â—‹ â—‹                 â—   *â—‹   â—‹ â—‹
     â—   â—‹â—‹  *â—‹                   â—   â—‹â—‹  *â—‹
```

## ğŸ”¬ PhiÃªn báº£n TECHNICAL

### Objective Function:

K-Means tá»‘i thiá»ƒu hÃ³a **Within-Cluster Sum of Squares (WCSS)**:

```
J = âˆ‘âˆ‘ ||xáµ¢ - Î¼â±¼||Â²
    j=1 iâˆˆCâ±¼
```

### Thuáº­t toÃ¡n Lloyd:

```python
def kmeans(X, k, max_iters=100):
    # Initialize centroids randomly
    centroids = initialize_random(X, k)

    for iteration in range(max_iters):
        # Assignment step
        clusters = assign_points_to_centroids(X, centroids)

        # Update step
        new_centroids = compute_centroids(clusters)

        # Check convergence
        if converged(centroids, new_centroids):
            break

        centroids = new_centroids

    return clusters, centroids
```

### Äá»™ phá»©c táº¡p:

- **Time**: O(n Ã— k Ã— i Ã— d)
  - n: sá»‘ Ä‘iá»ƒm dá»¯ liá»‡u
  - k: sá»‘ cá»¥m
  - i: sá»‘ iteration
  - d: sá»‘ chiá»u

### CÃ¡ch chá»n K:

1. **Elbow Method**: TÃ¬m "khuá»·u tay" trÃªn Ä‘á»“ thá»‹ WCSS
2. **Silhouette Analysis**: Äo Ä‘á»™ tÃ¡ch biá»‡t cá»§a cÃ¡c cá»¥m
3. **Gap Statistic**: So sÃ¡nh vá»›i phÃ¢n phá»‘i ngáº«u nhiÃªn

---

# 6. KOHONEN SOM - Báº¢N Äá»’ Tá»° Tá»” CHá»¨C

## ğŸŒŸ PhiÃªn báº£n Dá»„ HIá»‚U

### SOM lÃ  gÃ¬?

NhÆ° viá»‡c váº½ báº£n Ä‘á»“ tháº¿ giá»›i trÃªn giáº¥y pháº³ng! Biáº¿n dá»¯ liá»‡u phá»©c táº¡p nhiá»u chiá»u thÃ nh báº£n Ä‘á»“ 2D dá»… nhÃ¬n.

**VÃ­ dá»¥ thá»±c táº¿:**

```
ğŸµ PhÃ¢n loáº¡i nháº¡c:
- GÃ³c trÃªn trÃ¡i: Rock, Metal
- GÃ³c trÃªn pháº£i: Classical, Jazz
- GÃ³c dÆ°á»›i trÃ¡i: Pop, Dance
- GÃ³c dÆ°á»›i pháº£i: Country, Folk

â†’ Nháº¡c tÆ°Æ¡ng tá»± sáº½ á»Ÿ gáº§n nhau trÃªn báº£n Ä‘á»“!
```

### Táº¡i sao há»¯u Ã­ch?

- ğŸ‘ï¸ **Trá»±c quan hÃ³a**: Tháº¥y Ä‘Æ°á»£c pattern trong dá»¯ liá»‡u
- ğŸ—ºï¸ **Giáº£m chiá»u**: Tá»« 100 thuá»™c tÃ­nh â†’ báº£n Ä‘á»“ 2D
- ğŸ” **TÃ¬m nhÃ³m**: CÃ¡c vÃ¹ng tÆ°Æ¡ng tá»± nhau

### CÃ¡ch hoáº¡t Ä‘á»™ng (nhÆ° há»c báº£n Ä‘á»“):

1. **ğŸ—ºï¸ Táº¡o lÆ°á»›i trá»‘ng** (nhÆ° báº£n Ä‘á»“ tráº¯ng)
2. **ğŸ“ Äáº·t dá»¯ liá»‡u lÃªn lÆ°á»›i** (má»—i Ä‘iá»ƒm tÃ¬m vá»‹ trÃ­ phÃ¹ há»£p)
3. **ğŸ”„ Äiá»u chá»‰nh dáº§n** (vÃ¹ng xung quanh cÅ©ng thay Ä‘á»•i)
4. **âœ… HoÃ n thÃ nh báº£n Ä‘á»“** (dá»¯ liá»‡u tÆ°Æ¡ng tá»± á»Ÿ gáº§n nhau)

## ğŸ”¬ PhiÃªn báº£n TECHNICAL

### Cáº¥u trÃºc SOM:

- **Input Layer**: Vector Ä‘áº§u vÃ o x âˆˆ â„â¿
- **Output Layer**: LÆ°á»›i 2D cÃ¡c neuron vá»›i weight vector wáµ¢â±¼ âˆˆ â„â¿

### Thuáº­t toÃ¡n training:

**1. TÃ¬m Best Matching Unit (BMU):**

```
c = argmin ||x(t) - wáµ¢â±¼(t)||
    i,j
```

**2. Update weights:**

```
wáµ¢â±¼(t+1) = wáµ¢â±¼(t) + Î±(t) Ã— h(c,i,j,t) Ã— [x(t) - wáµ¢â±¼(t)]
```

**3. Neighborhood function:**

```
h(c,i,j,t) = exp(-||rc - ráµ¢â±¼||Â² / (2Ïƒ(t)Â²))
```

**4. Learning rate decay:**

```
Î±(t) = Î±â‚€ Ã— exp(-t/Ï„â‚)
Ïƒ(t) = Ïƒâ‚€ Ã— exp(-t/Ï„â‚‚)
```

### Pseudocode:

```python
def train_som(data, map_size, epochs):
    # Initialize weight vectors randomly
    weights = initialize_weights(map_size, input_dim)

    for epoch in range(epochs):
        for x in data:
            # Find BMU
            bmu = find_best_matching_unit(x, weights)

            # Update weights in neighborhood
            for i, j in map_coordinates:
                distance = euclidean_distance(bmu, (i,j))
                if distance <= neighborhood_radius(epoch):
                    influence = neighborhood_function(distance, epoch)
                    learning_rate = get_learning_rate(epoch)

                    weights[i][j] += learning_rate * influence * (x - weights[i][j])

    return weights
```

---

# ğŸ“Š SO SÃNH CÃC THUáº¬T TOÃN

| Thuáº­t toÃ¡n        | Loáº¡i           | á»¨ng dá»¥ng chÃ­nh        | Æ¯u Ä‘iá»ƒm           | NhÆ°á»£c Ä‘iá»ƒm           |
| ----------------- | -------------- | --------------------- | ----------------- | -------------------- |
| **Apriori**       | Association    | Basket Analysis       | TÃ¬m pattern áº©n    | Cháº­m vá»›i dá»¯ liá»‡u lá»›n |
| **Rough Set**     | Classification | Feature Selection     | Xá»­ lÃ½ uncertainty | Phá»©c táº¡p tÃ­nh toÃ¡n   |
| **Decision Tree** | Classification | Rule Extraction       | Dá»… hiá»ƒu           | Dá»… overfitting       |
| **Naive Bayes**   | Classification | Text Classification   | Nhanh, Ä‘Æ¡n giáº£n   | Giáº£ Ä‘á»‹nh Ä‘á»™c láº­p     |
| **K-Means**       | Clustering     | Customer Segmentation | ÄÆ¡n giáº£n, nhanh   | Cáº§n biáº¿t trÆ°á»›c K     |
| **SOM**           | Visualization  | Data Exploration      | Trá»±c quan hÃ³a tá»‘t | KhÃ³ diá»…n giáº£i        |

---

# ğŸ¯ Káº¾T LUáº¬N

Má»—i thuáº­t toÃ¡n cÃ³ Ä‘iá»ƒm máº¡nh riÃªng:

- **ğŸ›’ Muá»‘n tÃ¬m pattern mua hÃ ng** â†’ DÃ¹ng **Apriori**
- **ğŸ¯ Muá»‘n phÃ¢n loáº¡i chÃ­nh xÃ¡c** â†’ DÃ¹ng **Decision Tree** hoáº·c **Naive Bayes**
- **ğŸ‘¥ Muá»‘n chia nhÃ³m khÃ¡ch hÃ ng** â†’ DÃ¹ng **K-Means**
- **ğŸ—ºï¸ Muá»‘n khÃ¡m phÃ¡ dá»¯ liá»‡u** â†’ DÃ¹ng **SOM**
- **âš–ï¸ Muá»‘n loáº¡i bá» thuá»™c tÃ­nh thá»«a** â†’ DÃ¹ng **Rough Set**

**Lá»i khuyÃªn:** HÃ£y thá»­ nhiá»u thuáº­t toÃ¡n vÃ  so sÃ¡nh káº¿t quáº£ Ä‘á»ƒ chá»n phÆ°Æ¡ng phÃ¡p phÃ¹ há»£p nháº¥t!

---

_ğŸ“š TÃ i liá»‡u nÃ y Ä‘Æ°á»£c táº¡o Ä‘á»ƒ há»— trá»£ há»c táº­p mÃ´n Khai thÃ¡c Dá»¯ liá»‡u. ChÃºc báº¡n há»c tá»‘t! ğŸš€_
